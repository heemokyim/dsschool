{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bike Sharing Demand\n",
    "\n",
    "## 데이터분석과 시각화, 머신러닝 알고리즘으로 시간당 자전거 대여량을 예측하기\n",
    "\n",
    "이번 캐글 경진대회는 시간당 자전거 대여량을 예측하는 [Bike Sharing Demand](https://www.kaggle.com/c/bike-sharing-demand) 입니다. 워싱턴 D.C 소재의 자전거 대여 스타트업 [Capital Bikeshare](https://www.capitalbikeshare.com/)의 데이터를 활용하여, 특정 시간대에 얼마나 많은 사람들이 자전거를 대여하는지 예측하는 것이 목표입니다.\n",
    "\n",
    "사람들이 자전거를 대여하는데는 많은 요소가 관여되어 있을 겁니다. 가령 시간(새벽보다 낮에 많이 빌리겠죠), 날씨(비가 오면 자전거를 대여하지 않을 겁니다), 근무일(근무 시간에는 자전거를 대여하지 않겠죠) 등. 이런 모든 요소를 조합하여 워싱턴 D.C의 자전거 교통량을 예측해주세요. 이번 경진대회에서는 기존까지 배웠던 프로그래밍 언어와 인공지능&머신러닝 능력 외에도, 자전거 렌탈 시장에 대한 약간의 전문지식, 그리고 일반인의 기초 상식을 총동원 할 수 있습니다.\n",
    "\n",
    "저번 [Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic/) 경진대회와 마찬가지로, 이번에도 프로그래밍 언어 파이썬([Python](https://www.python.org/)), 데이터 분석 패키지 판다스([Pandas](https://pandas.pydata.org/)), 그리고 머신러닝&인공지능 라이브러리인 싸이킷런([scikit-learn](scikit-learn.org))을 사용합니다. 여기에 더불어, 이번에는 데이터 시각화 패키지 [matplotlib](https://matplotlib.org/)와 [Seaborn](https://seaborn.pydata.org/)을 본격적으로 활용해볼 것입니다.\n",
    "\n",
    "## 컬럼 설명\n",
    "\n",
    "(데이터는 [다음의 링크](https://www.kaggle.com/c/bike-sharing-demand/data)에서 다운받으실 수 있습니다)\n",
    "\n",
    "  * **datetime** - 시간. 연-월-일 시:분:초 로 표현합니다. (가령 2011-01-01 00:00:00은 2011년 1월 1일 0시 0분 0초)\n",
    "  * **season** - 계절. 봄(1), 여름(2), 가을(3), 겨울(4) 순으로 표현합니다.\n",
    "  * **holiday** - 공휴일. 1이면 공휴일이며, 0이면 공휴일이 아닙니다.\n",
    "  * **workingday** - 근무일. 1이면 근무일이며, 0이면 근무일이 아닙니다.\n",
    "  * **weather** - 날씨. 1 ~ 4 사이의 값을 가지며, 구체적으로는 다음과 같습니다.\n",
    "    * 1: 아주 깨끗한 날씨입니다. 또는 아주 약간의 구름이 끼어있습니다.\n",
    "    * 2: 약간의 안개와 구름이 끼어있는 날씨입니다.\n",
    "    * 3: 약간의 눈, 비가 오거나 천둥이 칩니다.\n",
    "    * 4: 아주 많은 비가 오거나 우박이 내립니다.\n",
    "  * **temp** - 온도. 섭씨(Celsius)로 적혀있습니다.\n",
    "  * **atemp** - 체감 온도. 마찬가지로 섭씨(Celsius)로 적혀있습니다.\n",
    "  * **humidity** - 습도.\n",
    "  * **windspeed** - 풍속.\n",
    "  * **casual** - 비회원(non-registered)의 자전거 대여량.\n",
    "  * **registered** - 회원(registered)의 자전거 대여량.\n",
    "  * **count** - 총 자전거 대여랑. 비회원(casual) + 회원(registered)과 동일합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 파이썬의 데이터 분석 패키지 Pandas(pandas.pydata.org) 를 읽어옵니다.\n",
    "# Pandas는 쉽게 말해 파이썬으로 엑셀을 다룰 수 있는 툴이라고 보시면 됩니다.\n",
    "# 이 패키지를 앞으로는 pd라는 축약어로 사용하겠습니다.\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "\n",
    "언제나처럼 모든 데이터 분석의 시작은 주어진 데이터를 읽어오는 것입니다. [판다스(Pandas)](https://pandas.pydata.org/)의 [read_csv](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html)를 활용하여 [Bike Sharing Demand](https://www.kaggle.com/c/bike-sharing-demand) 경진대회에서 제공하는 두 개의 데이터(train, test)를 읽어오겠습니다. ([다운로드 링크](https://www.kaggle.com/c/bike-sharing-demand/data))\n",
    "\n",
    "앞서 [Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic/) 경진대회와 마찬가지로, 여기에서도 파일의 경로를 지정하는 방법에 주의하셔야 합니다. 만일 read_csv를 실행할 때 (**FileNotFoundError**)라는 이름의 에러가 난다면 경로가 제대로 지정이 되지 않은 것입니다. **파일의 경로를 지정하는 법이 생각나지 않는다면 [다음의 링크](http://88240.tistory.com/122)를 통해 경로를 지정하는 법을 복습한 뒤 다시 시도해주세요.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10886, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>season</th>\n",
       "      <th>holiday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weather</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>humidity</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-01-01 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.84</td>\n",
       "      <td>14.395</td>\n",
       "      <td>81</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-01-01 01:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.02</td>\n",
       "      <td>13.635</td>\n",
       "      <td>80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-01-01 02:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.02</td>\n",
       "      <td>13.635</td>\n",
       "      <td>80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-01-01 03:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.84</td>\n",
       "      <td>14.395</td>\n",
       "      <td>75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-01-01 04:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.84</td>\n",
       "      <td>14.395</td>\n",
       "      <td>75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             datetime  season  holiday  workingday  weather  temp   atemp  \\\n",
       "0 2011-01-01 00:00:00       1        0           0        1  9.84  14.395   \n",
       "1 2011-01-01 01:00:00       1        0           0        1  9.02  13.635   \n",
       "2 2011-01-01 02:00:00       1        0           0        1  9.02  13.635   \n",
       "3 2011-01-01 03:00:00       1        0           0        1  9.84  14.395   \n",
       "4 2011-01-01 04:00:00       1        0           0        1  9.84  14.395   \n",
       "\n",
       "   humidity  windspeed  casual  registered  count  \n",
       "0        81        0.0       3          13     16  \n",
       "1        80        0.0       8          32     40  \n",
       "2        80        0.0       5          27     32  \n",
       "3        75        0.0       3          10     13  \n",
       "4        75        0.0       0           1      1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 판다스의 read_csv로 train.csv 파일을 읽어옵니다.\n",
    "# 여기서 datetime은 특별히 날짜로 해석하기 위해 parse_dates 옵션에 넣어줍니다.\n",
    "# 읽어온 데이터를 train이라는 이름의 변수에 할당합니다.\n",
    "train = pd.read_csv(\"data/bike/train.csv\", parse_dates=[\"datetime\"])\n",
    "\n",
    "# train 변수에 할당된 데이터의 행렬 사이즈를 출력합니다.\n",
    "# 출력은 (row, column) 으로 표시됩니다.\n",
    "print(train.shape)\n",
    "\n",
    "# head()로 train 데이터의 상위 5개를 띄웁니다.\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6493, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>season</th>\n",
       "      <th>holiday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weather</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>humidity</th>\n",
       "      <th>windspeed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-01-20 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10.66</td>\n",
       "      <td>11.365</td>\n",
       "      <td>56</td>\n",
       "      <td>26.0027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-01-20 01:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10.66</td>\n",
       "      <td>13.635</td>\n",
       "      <td>56</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-01-20 02:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10.66</td>\n",
       "      <td>13.635</td>\n",
       "      <td>56</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-01-20 03:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10.66</td>\n",
       "      <td>12.880</td>\n",
       "      <td>56</td>\n",
       "      <td>11.0014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-01-20 04:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10.66</td>\n",
       "      <td>12.880</td>\n",
       "      <td>56</td>\n",
       "      <td>11.0014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             datetime  season  holiday  workingday  weather   temp   atemp  \\\n",
       "0 2011-01-20 00:00:00       1        0           1        1  10.66  11.365   \n",
       "1 2011-01-20 01:00:00       1        0           1        1  10.66  13.635   \n",
       "2 2011-01-20 02:00:00       1        0           1        1  10.66  13.635   \n",
       "3 2011-01-20 03:00:00       1        0           1        1  10.66  12.880   \n",
       "4 2011-01-20 04:00:00       1        0           1        1  10.66  12.880   \n",
       "\n",
       "   humidity  windspeed  \n",
       "0        56    26.0027  \n",
       "1        56     0.0000  \n",
       "2        56     0.0000  \n",
       "3        56    11.0014  \n",
       "4        56    11.0014  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train.csv 파일을 읽어온 방식과 동일하게 test.csv를 읽어옵니다.\n",
    "# 이후 이 데이터를 test라는 이름의 변수에 저장합니다.\n",
    "test = pd.read_csv(\"data/bike/test.csv\", parse_dates=[\"datetime\"])\n",
    "\n",
    "# 마찬가지로 행렬(row, column) 사이즈를 출력하고\n",
    "print(test.shape)\n",
    "\n",
    "# 전체 test 데이터에서 상위 5개만 출력합니다.\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "데이터를 읽어왔으면, 이 데이터를 편하게 분석하고 머신러닝 알고리즘에 집어넣기 위해 간단한 전처리(Preprocessing) 작업을 진행하겠습니다.\n",
    "\n",
    "[Bike Sharing Demand](https://www.kaggle.com/c/bike-sharing-demand)는 편리하게도 대부분의 데이터가 전처리 되어있습니다. (가령 season 컬럼은 봄을 spring이라 표현하지 않고 1이라고 표현합니다) 그러므로 [Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic/) 경진대회와는 달리 간단한 전처리만 끝내면 바로 머신러닝 모델에 데이터를 집어넣을 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse datetime\n",
    "\n",
    "먼저 **날짜(datetime)** 컬럼을 전처리 하겠습니다.\n",
    "\n",
    "날짜 컬럼은 얼핏 보면 여러개의 숫자로 구성되어 있습니다. (ex: 2011-01-01 00:00:00) 하지만 결론적으로 숫자는 아니며, 판다스에서는 문자열(object) 또는 날짜(datetime64)로 인식합니다. (값에 하이픈(-)과 콜론(:)이 있기 때문입니다) 그러므로 날짜(datetime) 컬럼을 사용하기 위해서는 머신러닝 알고리즘이 이해할 수 있는 방식으로 전처리를 해줘야 합니다.\n",
    "\n",
    "날짜(datetime) 컬럼을 전처리하는 가장 쉬운 방법은 연, 월, 일, 시, 분, 초를 따로 나누는 것입니다. 가령 2011-01-01 00:00:00은 2011년 1월 1일 0시 0분 0초라고 볼 수 있으므로, 2011, 1, 1, 0, 0, 0으로 따로 나누면 총 6개의 숫자가 됩니다. 즉, **날짜(datetime) 컬럼을 여섯개의 다른 컬럼으로 나누어주는 것이 날짜 컬럼을 전처리하는 핵심입니다**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10886, 19)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>datetime-year</th>\n",
       "      <th>datetime-month</th>\n",
       "      <th>datetime-day</th>\n",
       "      <th>datetime-hour</th>\n",
       "      <th>datetime-minute</th>\n",
       "      <th>datetime-second</th>\n",
       "      <th>datetime-dayofweek</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-01-01 00:00:00</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-01-01 01:00:00</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-01-01 02:00:00</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-01-01 03:00:00</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-01-01 04:00:00</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             datetime  datetime-year  datetime-month  datetime-day  \\\n",
       "0 2011-01-01 00:00:00           2011               1             1   \n",
       "1 2011-01-01 01:00:00           2011               1             1   \n",
       "2 2011-01-01 02:00:00           2011               1             1   \n",
       "3 2011-01-01 03:00:00           2011               1             1   \n",
       "4 2011-01-01 04:00:00           2011               1             1   \n",
       "\n",
       "   datetime-hour  datetime-minute  datetime-second  datetime-dayofweek  \n",
       "0              0                0                0                   5  \n",
       "1              1                0                0                   5  \n",
       "2              2                0                0                   5  \n",
       "3              3                0                0                   5  \n",
       "4              4                0                0                   5  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train 데이터에 연, 월, 일, 시, 분, 초를 나타내는 새로운 컬럼을 생성합니다.\n",
    "# 각각의 이름을 datetime-year/month/day/hour/minute/second라고 가정합니다.\n",
    "# 이 컬럼에 날짜(datetime) 컬럼의 dt(datetime의 약자입니다) 옵션을 활용하여 연월일시분초를 따로 넣어줍니다.\n",
    "train[\"datetime-year\"] = train[\"datetime\"].dt.year\n",
    "train[\"datetime-month\"] = train[\"datetime\"].dt.month\n",
    "train[\"datetime-day\"] = train[\"datetime\"].dt.day\n",
    "train[\"datetime-hour\"] = train[\"datetime\"].dt.hour\n",
    "train[\"datetime-minute\"] = train[\"datetime\"].dt.minute\n",
    "train[\"datetime-second\"] = train[\"datetime\"].dt.second\n",
    "\n",
    "# dayofweek는 날짜에서 요일(월~일)을 가져오는 기능입니다.\n",
    "# 값은 0(월), 1(화), 2(수), 3(목), 4(금), 5(토), 6(일) 을 나타냅니다.\n",
    "train[\"datetime-dayofweek\"] = train[\"datetime\"].dt.dayofweek\n",
    "\n",
    "# train 변수에 할당된 데이터의 행렬 사이즈를 출력합니다.\n",
    "# 출력은 (row, column) 으로 표시됩니다.\n",
    "print(train.shape)\n",
    "\n",
    "# .head()로 train 데이터의 상위 5개를 띄우되,\n",
    "# datetime과 이와 연관된 나머지 일곱 개의 컬럼만을 출력합니다.\n",
    "train[[\"datetime\", \"datetime-year\", \"datetime-month\", \"datetime-day\", \"datetime-hour\", \"datetime-minute\", \"datetime-second\", \"datetime-dayofweek\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10886, 20)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>datetime-dayofweek</th>\n",
       "      <th>datetime-dayofweek(humanized)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-01-01 00:00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>Saturday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-01-01 01:00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>Saturday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-01-01 02:00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>Saturday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-01-01 03:00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>Saturday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-01-01 04:00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>Saturday</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             datetime  datetime-dayofweek datetime-dayofweek(humanized)\n",
       "0 2011-01-01 00:00:00                   5                      Saturday\n",
       "1 2011-01-01 01:00:00                   5                      Saturday\n",
       "2 2011-01-01 02:00:00                   5                      Saturday\n",
       "3 2011-01-01 03:00:00                   5                      Saturday\n",
       "4 2011-01-01 04:00:00                   5                      Saturday"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# datetime-dayofweek를 사람이 이해하기 쉬운 표현으로 변경합니다. (Monday ~ Sunday)\n",
    "# 이를 datetime-dayofweek(humanized)라는 새로운 컬럼에 추가합니다.\n",
    "train.loc[train[\"datetime-dayofweek\"] == 0, \"datetime-dayofweek(humanized)\"] = \"Monday\"\n",
    "train.loc[train[\"datetime-dayofweek\"] == 1, \"datetime-dayofweek(humanized)\"] = \"Tuesday\"\n",
    "train.loc[train[\"datetime-dayofweek\"] == 2, \"datetime-dayofweek(humanized)\"] = \"Wednesday\"\n",
    "train.loc[train[\"datetime-dayofweek\"] == 3, \"datetime-dayofweek(humanized)\"] = \"Thursday\"\n",
    "train.loc[train[\"datetime-dayofweek\"] == 4, \"datetime-dayofweek(humanized)\"] = \"Friday\"\n",
    "train.loc[train[\"datetime-dayofweek\"] == 5, \"datetime-dayofweek(humanized)\"] = \"Saturday\"\n",
    "train.loc[train[\"datetime-dayofweek\"] == 6, \"datetime-dayofweek(humanized)\"] = \"Sunday\"\n",
    "\n",
    "# train 변수에 할당된 데이터의 행렬 사이즈를 출력합니다.\n",
    "# 출력은 (row, column) 으로 표시됩니다.\n",
    "print(train.shape)\n",
    "\n",
    "# .head()로 train 데이터의 상위 5개를 띄우되,\n",
    "# datetime과 datetime-dayofweek, 그리고 datetime-dayofweek(humanized) 컬럼만을 출력합니다.\n",
    "train[[\"datetime\", \"datetime-dayofweek\", \"datetime-dayofweek(humanized)\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6493, 16)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>datetime-year</th>\n",
       "      <th>datetime-month</th>\n",
       "      <th>datetime-day</th>\n",
       "      <th>datetime-hour</th>\n",
       "      <th>datetime-minute</th>\n",
       "      <th>datetime-second</th>\n",
       "      <th>datetime-dayofweek</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-01-20 00:00:00</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-01-20 01:00:00</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-01-20 02:00:00</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-01-20 03:00:00</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-01-20 04:00:00</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             datetime  datetime-year  datetime-month  datetime-day  \\\n",
       "0 2011-01-20 00:00:00           2011               1            20   \n",
       "1 2011-01-20 01:00:00           2011               1            20   \n",
       "2 2011-01-20 02:00:00           2011               1            20   \n",
       "3 2011-01-20 03:00:00           2011               1            20   \n",
       "4 2011-01-20 04:00:00           2011               1            20   \n",
       "\n",
       "   datetime-hour  datetime-minute  datetime-second  datetime-dayofweek  \n",
       "0              0                0                0                   3  \n",
       "1              1                0                0                   3  \n",
       "2              2                0                0                   3  \n",
       "3              3                0                0                   3  \n",
       "4              4                0                0                   3  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test 데이터와 train 데이터와 동일하게 연, 월, 일, 시, 분, 초 컬럼을 생성합니다.\n",
    "test[\"datetime-year\"] = test[\"datetime\"].dt.year\n",
    "test[\"datetime-month\"] = test[\"datetime\"].dt.month\n",
    "test[\"datetime-day\"] = test[\"datetime\"].dt.day\n",
    "test[\"datetime-hour\"] = test[\"datetime\"].dt.hour\n",
    "test[\"datetime-minute\"] = test[\"datetime\"].dt.minute\n",
    "test[\"datetime-second\"] = test[\"datetime\"].dt.second\n",
    "\n",
    "# dayofweek 컬럼도 train 데이터와 동일하게 생성합니다.\n",
    "test[\"datetime-dayofweek\"] = test[\"datetime\"].dt.dayofweek\n",
    "\n",
    "# test 변수에 할당된 데이터의 행렬 사이즈를 출력합니다.\n",
    "# 출력은 (row, column) 으로 표시됩니다.\n",
    "print(test.shape)\n",
    "\n",
    "# .head()로 test 데이터의 상위 5개를 띄우되,\n",
    "# datetime과 이와 연관된 나머지 일곱 개의 컬럼만을 출력합니다.\n",
    "test[[\"datetime\", \"datetime-year\", \"datetime-month\", \"datetime-day\", \"datetime-hour\", \"datetime-minute\", \"datetime-second\", \"datetime-dayofweek\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6493, 17)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>datetime-dayofweek</th>\n",
       "      <th>datetime-dayofweek(humanized)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-01-20 00:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>Thursday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-01-20 01:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>Thursday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-01-20 02:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>Thursday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-01-20 03:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>Thursday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-01-20 04:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>Thursday</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             datetime  datetime-dayofweek datetime-dayofweek(humanized)\n",
       "0 2011-01-20 00:00:00                   3                      Thursday\n",
       "1 2011-01-20 01:00:00                   3                      Thursday\n",
       "2 2011-01-20 02:00:00                   3                      Thursday\n",
       "3 2011-01-20 03:00:00                   3                      Thursday\n",
       "4 2011-01-20 04:00:00                   3                      Thursday"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# datetime-dayofweek를 사람이 이해하기 쉬운 표현으로 변경합니다. (Monday ~ Sunday)\n",
    "# 이를 datetime-dayofweek(humanized)라는 새로운 컬럼에 추가합니다.\n",
    "test.loc[test[\"datetime-dayofweek\"] == 0, \"datetime-dayofweek(humanized)\"] = \"Monday\"\n",
    "test.loc[test[\"datetime-dayofweek\"] == 1, \"datetime-dayofweek(humanized)\"] = \"Tuesday\"\n",
    "test.loc[test[\"datetime-dayofweek\"] == 2, \"datetime-dayofweek(humanized)\"] = \"Wednesday\"\n",
    "test.loc[test[\"datetime-dayofweek\"] == 3, \"datetime-dayofweek(humanized)\"] = \"Thursday\"\n",
    "test.loc[test[\"datetime-dayofweek\"] == 4, \"datetime-dayofweek(humanized)\"] = \"Friday\"\n",
    "test.loc[test[\"datetime-dayofweek\"] == 5, \"datetime-dayofweek(humanized)\"] = \"Saturday\"\n",
    "test.loc[test[\"datetime-dayofweek\"] == 6, \"datetime-dayofweek(humanized)\"] = \"Sunday\"\n",
    "\n",
    "# test 변수에 할당된 데이터의 행렬 사이즈를 출력합니다.\n",
    "# 출력은 (row, column) 으로 표시됩니다.\n",
    "print(test.shape)\n",
    "\n",
    "# .head()로 test 데이터의 상위 5개를 띄우되,\n",
    "# datetime과 datetime-dayofweek, 그리고 datetime-dayofweek(humanized) 컬럼만을 출력합니다.\n",
    "test[[\"datetime\", \"datetime-dayofweek\", \"datetime-dayofweek(humanized)\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore\n",
    "\n",
    "전처리(Preprocesing)를 끝냈으면 그 다음에는 데이터를 분석해보겠습니다.\n",
    "\n",
    "주어진 데이터를 시각화나 분석 툴을 통해 다양한 관점에서 이해하는 과정을 탐험적 데이터 분석([Exploratory Data Analysis](https://en.wikipedia.org/wiki/Exploratory_data_analysis))이라고 합니다. 저번 타이타닉 문제와 마찬가지로, 이번에도 파이썬의 데이터 시각화 패키지인 ([matplotlib](https://matplotlib.org))와 [seaborn](https://seaborn.pydata.org/) 을 활용해서 분석해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# matplotlib로 실행하는 모든 시각화를 자동으로 쥬피터 노트북에 띄웁니다.\n",
    "# seaborn 도 결국에는 matplotlib를 기반으로 동작하기 때문에, seaborn으로 실행하는 모든 시각화도 마찬가지로 쥬피터 노트북에 자동적으로 띄워집니다.\n",
    "%matplotlib inline\n",
    "\n",
    "# 데이터 시각화 패키지 seaborn을 로딩합니다. 앞으로는 줄여서 sns라고 사용할 것입니다.\n",
    "import seaborn as sns\n",
    "\n",
    "# 데이터 시각화 패키지 matplotlib를 로딩합니다. 앞으로는 줄여서 plt라고 사용할 것입니다.\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### datetime\n",
    "\n",
    "먼저 분석할 컬럼은 **날짜(datetime)** 컬럼입니다. 날짜 컬럼은 [Bike Sharing Demand](https://www.kaggle.com/c/bike-sharing-demand) 경진대회의 핵심 컬럼이라고 볼 수 있으며, 이번 경진대회에서 상위 성적을 올리고 싶다면 날짜 컬럼을 완벽하게 이해하는 것이 무엇보다도 중요합니다.\n",
    "\n",
    "먼저 연/월/일/시/분/초에 따른 자전거 대여량을 시각화 해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f3902e753c8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABCYAAAHjCAYAAAD/th6fAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f390507e4e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# matplotlib의 subplots를 사용합니다. 이 함수는 여러 개의 시각화를 한 화면에 띄울 수 있도록 합니다.\n",
    "# 이번에는 2x3으로 총 6개의 시각화를 한 화면에 띄웁니다.\n",
    "figure, ((ax1, ax2, ax3), (ax4, ax5, ax6)) = plt.subplots(nrows=2, ncols=3)\n",
    "\n",
    "# 시각화의 전체 사이즈는 18x8로 설정합니다.\n",
    "figure.set_size_inches(18, 8)\n",
    "\n",
    "# seaborn의 barplot으로 subplots의 각 구역에\n",
    "# 연, 월, 일, 시, 분, 초 별 자전거 대여량을 출력합니다.\n",
    "sns.barplot(data=train, x=\"datetime-year\", y=\"count\", ax=ax1)\n",
    "sns.barplot(data=train, x=\"datetime-month\", y=\"count\", ax=ax2)\n",
    "sns.barplot(data=train, x=\"datetime-day\", y=\"count\", ax=ax3)\n",
    "sns.barplot(data=train, x=\"datetime-hour\", y=\"count\", ax=ax4)\n",
    "sns.barplot(data=train, x=\"datetime-minute\", y=\"count\", ax=ax5)\n",
    "sns.barplot(data=train, x=\"datetime-second\", y=\"count\", ax=ax6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 그림에서 알 수 있는 내용은 다음과 같습니다.\n",
    "\n",
    "**datetime-year**\n",
    "  * 2011년도의 자전거 대여량보다 2012년도의 자전거 대여량이 더 높습니다. 이는 [Bike Sharing Demand](https://www.kaggle.com/c/bike-sharing-demand) 경진대회를 주최한 [Capital Bikeshare](https://www.capitalbikeshare.com/)사가 꾸준히 성장하고 있다고 간주할 수 있습니다.\n",
    "\n",
    "**datetime-month**\n",
    "  * 주로 여름(6~8월)에 자전거를 많이 빌리며, 겨울(12~2월)에는 자전거를 많이 빌리지 않습니다.\n",
    "  * 같은 겨울이라도 12월의 자전거 대여량이 1월의 자전거 대여량보다 두 배 가까이 높아 보입니다. 하지만 여기에는 숨겨진 비밀이 있는데, 다음에 나올 다른 시각화에서 자세히 살펴보겠습니다.\n",
    "  \n",
    "**datetime-day**\n",
    "  * x축을 자세히 보면 1일부터 19일까지밖에 없습니다. 20일은 어디에 있을까요? 바로 test 데이터에 있습니다. 이 시각화에서 알 수 있는 내용은, train 데이터와 test 데이터를 나누는 기준이 되는 컬럼이 바로 **datetime-day**라는 것입니다.\n",
    "  * 이런 경우 **datetime-day**를 feature로 집어넣으면 머신러닝 알고리즘이 과적합([overfitting](https://hyperdot.wordpress.com/2017/02/06/%EA%B3%BC%EC%A0%81%ED%95%A9overfitting/)) 되는 현상이 일어날 수 있습니다. **그러므로 train 데이터와 test 데이터를 나누는 기준이 되는 컬럼이 있으면, 이 컬럼은 feature로 사용하지 않는 것이 좋습니다.**\n",
    "\n",
    "**datetime-hour**\n",
    "  * 새벽 시간에는 사람들이 자전거를 빌리지 않으며, 오후 시간에 상대적으로 자전거를 많이 빌립니다.\n",
    "  * 특이하게도 두 부분에서 사람들이 자전거를 특별히 많이 빌리는 현상이 있습니다. 바로 출근 시간(7~9시)과 퇴근 시간(16시~19시) 입니다.\n",
    "  * 물론 출퇴근시간이 아닌 다른 시간대에 자전거를 빌리는 경우도 존재합니다. 이는 다음에 나올 다른 시각화에서 자세히 살펴보겠습니다.\n",
    "\n",
    "**datetime-minute** & **datetime-second**\n",
    "  * 이 두 컬럼은 x축이 모두 0으로 되어있습니다. 즉, **datetime-minute**과 **datetime-second**은 기록되고 있지 않다는 사실을 알 수 있습니다. 이 경우에는 feature로 넣어도 큰 의미가 없기 때문에 사용하지 않습니다.\n",
    "  \n",
    "  \n",
    "그러므로 이 시각화에서 알 수 있는 결론은, **전체 여섯개의 컬럼 중 datetime-year와 datetime-month, 그리고 datetime-hour만 사용하는 것이 가장 좋다는 사실을 깨달을 수 있습니다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### datetime-year & datetime-month\n",
    "\n",
    "다음에는 연-월을 붙여서 시각화해보겠습니다.\n",
    "\n",
    "이전에는 연/월을 따로 시각화해서 출력하였지만, 이번에는 연-월을 붙여서 2011년 1월부터 2012년 12월까지 총 24개의 경우의 수를 x축으로 놓고 시각화해보고 싶습니다. 먼저 이를 시각화하기에 필요한 **datetime-year_month**라는 새로운 컬럼을 만들어 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10886, 23)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>datetime-year_month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-01-01 00:00:00</td>\n",
       "      <td>2011-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-01-01 01:00:00</td>\n",
       "      <td>2011-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-01-01 02:00:00</td>\n",
       "      <td>2011-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-01-01 03:00:00</td>\n",
       "      <td>2011-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-01-01 04:00:00</td>\n",
       "      <td>2011-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             datetime datetime-year_month\n",
       "0 2011-01-01 00:00:00              2011-1\n",
       "1 2011-01-01 01:00:00              2011-1\n",
       "2 2011-01-01 02:00:00              2011-1\n",
       "3 2011-01-01 03:00:00              2011-1\n",
       "4 2011-01-01 04:00:00              2011-1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# datetime-year와 datetime-month의 형태를 변환합니다.\n",
    "# 이전까지는 정수형(int)였지만, pandas의 astype을 통해 문자열(str)로 변환합니다.\n",
    "# 이 결과를 datetime-year(str)와 datetime-month(str)라는 새로운 컬럼에 집어넣습니다.\n",
    "train[\"datetime-year(str)\"] = train[\"datetime-year\"].astype('str')\n",
    "train[\"datetime-month(str)\"] = train[\"datetime-month\"].astype('str')\n",
    "\n",
    "# datetime-year(str)와 datetime-month(str) 문자열 두 개를 붙여서 datetime-year_month라는 새로운 컬럼을 추가합니다.\n",
    "# 이 컬럼에는 2011-1부터 2012-12까지의 총 24의 경우의 수가 들어갑니다.\n",
    "train[\"datetime-year_month\"] = train[\"datetime-year(str)\"] + \"-\" + train[\"datetime-month(str)\"]\n",
    "\n",
    "# train 변수에 할당된 데이터의 행렬 사이즈를 출력합니다.\n",
    "# 출력은 (row, column) 으로 표시됩니다.\n",
    "print(train.shape)\n",
    "\n",
    "# .head() train 데이터의 상위 5개를 띄우되,\n",
    "# datetime과 datetime-year_month 두 개의 컬럼만 출력합니다.\n",
    "train[[\"datetime\", \"datetime-year_month\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f39024c5e80>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABCYAAAEKCAYAAADO/VTVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJEmSJElSM/8fcwXmoYdm/ZYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f39029ec780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABCYAAAELCAYAAAAFoYdwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f39029fe710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# matplotlib의 subplots를 사용합니다. 이 함수는 여러 개의 시각화를 한 화면에 띄울 수 있도록 합니다.\n",
    "# 이번에는 1x2로 총 2개의 시각화를 한 화면에 띄웁니다.\n",
    "figure, (ax1, ax2) = plt.subplots(nrows=1, ncols=2)\n",
    "\n",
    "# 시각화의 전체 사이즈는 18x4로 설정합니다.\n",
    "figure.set_size_inches(18, 4)\n",
    "\n",
    "# seaborn의 barplot으로 subplots의 각 구역에\n",
    "# 연, 월별 자전거 대여량을 출력합니다.\n",
    "sns.barplot(data=train, x=\"datetime-year\", y=\"count\", ax=ax1)\n",
    "sns.barplot(data=train, x=\"datetime-month\", y=\"count\", ax=ax2)\n",
    "\n",
    "# 다시 한 번 matplotlib의 subplots를 사용합니다.\n",
    "# 이번에는 1x1로 1개의 시각화만을 출력합니다.\n",
    "figure, ax3 = plt.subplots(nrows=1, ncols=1)\n",
    "\n",
    "# 이 시각화의 전체 사이즈는 18x4로 설정합니다.\n",
    "figure.set_size_inches(18, 4)\n",
    "\n",
    "# 이번에는 seaborn의 barplot으로 연-월을 붙여서 자전거 대여량을 출력합니다.\n",
    "sns.barplot(data=train, x=\"datetime-year_month\", y=\"count\", ax=ax3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 그림에서 알 수 있는 내용은 다음과 같습니다.\n",
    "\n",
    "  * [Capital Bikeshare](https://www.capitalbikeshare.com/)사의 자전거 대여량은 꾸준히 상승하고 있는 추세입니다.\n",
    "  * 우상단 시각화를 보자면, 12월의 자전거 대여량이 1월의 자전거 대여량보다 두 배 가까이 높습니다.\n",
    "  * 하지만 아래의 시각화를 보면, 2011년 12월의 자전거 대여량과 2012년 1월의 자전거 대여량이 큰 차이가 없다는 사실을 발견할 수 있습니다.\n",
    "  * 반면에 2011년 1월의 자전거 대여량과 2012년 12월의 자전거 대여량은 큰 차이가 나는 것을 알 수 있습니다.\n",
    "  \n",
    "즉, 12월이 1월에 비해 자전거 대여량이 두 배 가까이 높은 이유는, 1) [Capital Bikeshare](https://www.capitalbikeshare.com/)의 자전거 대여량이 꾸준히 상승하고 있는 추세이며, 2) 이 과정에서 시기상으로 12월이 1월부터 늦게 발생했기 때문입니다. 즉 **자전거를 대여하는 고객 입장에서 12월이라고 자전거를 더 많이 빌려야 할 이유는 없습니다.**\n",
    "\n",
    "이 점 역시 머신러닝 알고리즘이 과적합(overfitting)될 소지가 다분합니다. 이를 해결할 수 있는 다양한 방법이 있는데,\n",
    "\n",
    "  * **datetime-year_month**를 통채로 One Hot Encoding해서 feature로 사용한다.\n",
    "  * 자전거 대여량이 꾸준히 성장하는 추세에 맞춰서 count를 보정한다.\n",
    "  \n",
    "하지만 제 경험상, 가장 쉽과 빠르게 머신러닝 모델의 정확도를 늘리는 방법은 **datetime-month**를 feature로 사용하지 않는 것입니다. 그러므로 **연/월/일/시/분/초 여섯 개의 컬럼 중 연도(datetime-year)와 시간(datetime-hour), 이렇게 두 개의 컬럼만 사용하도록 하겠습니다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### datetime-hour\n",
    "\n",
    "다음에는 **datetime-hour** 컬럼을 분석해보겠습니다.\n",
    "\n",
    "이번에는 **datetime-hour** 컬럼 외에도 두 개의 컬럼을 추가로 분석하겠습니다. 바로 근무일(workingday)와 요일(datetime-dayofweek)입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f39027ca5c0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABCYAAAK9CAYAAAD42m2FAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nShktTCillFJKKaWUUipl/j8sdDsVCaJUtgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f39029eca58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# matplotlib의 subplots를 사용합니다. 이 함수는 여러 개의 시각화를 한 화면에 띄울 수 있도록 합니다.\n",
    "# 이번에는 3x1로 총 3개의 시각화를 한 화면에 띄웁니다.\n",
    "figure, (ax1, ax2, ax3) = plt.subplots(nrows=3, ncols=1)\n",
    "\n",
    "# 시각화의 전체 사이즈는 18x12로 설정합니다.\n",
    "figure.set_size_inches(18, 12)\n",
    "\n",
    "# seaborn의 pointplot으로 시각당 자전거 대여량을 시각화합니다.\n",
    "sns.pointplot(data=train, x=\"datetime-hour\", y=\"count\", ax=ax1)\n",
    "\n",
    "# 비슷하게 seaborn의 pointplot으로 시각당 자전거 대여량을 시각화합니다.\n",
    "# 하지만 이번에는 근무일(workingday)에 따른 차이를 보여줍니다.\n",
    "sns.pointplot(data=train, x=\"datetime-hour\", y=\"count\", hue=\"workingday\", ax=ax2)\n",
    "\n",
    "# 비슷하게 seaborn의 pointplot으로 시각당 자전거 대여량을 시각화합니다.\n",
    "# 하지만 이번에는 요일(datetime-dayofweek)에 따른 차이를 보여줍니다.\n",
    "sns.pointplot(data=train, x=\"datetime-hour\", y=\"count\", hue=\"datetime-dayofweek(humanized)\", ax=ax3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 그림에서 알 수 있는 내용은 다음과 같습니다.\n",
    "\n",
    "  * 사람들은 기본적으로 출근 시간(7~9시)과 퇴근 시간(16~19시)에 자전거를 많이 빌립니다.\n",
    "  * 하지만 이는 근무일일 경우(workingday == 1)에만 한정된 이야기입니다. 근무일이 아닐 경우(workingday == 0), 사람들은 출/퇴근시간에 자전거를 빌리지 않고, 오후 시간(10 ~ 16시)에 자전거를 많이 빌리는 것을 확인할 수 있습니다.\n",
    "\n",
    "이번에는 **요일(datetime-dayofweek)**별 자전거 대여량을 살펴보겠습니다.\n",
    "\n",
    "  * 먼저 금요일을 살펴보면, 다른 주중(월~목)에 비해 퇴근 시간(17~19시)에 상대적으로 자전거를 덜 빌리는 사실을 알 수 있습니다. 이는 추측컨데 모종의 이유로 자전거를 탈 수 없거나(ex: 음주), 다른 교통수단(ex: 버스, 택시)을 대신 사용했다는 것을 알 수 있습니다.\n",
    "  * 반면 금요일은 주중임에도 불구하고 상대적으로 오후 시간(10~16시)의 자전거 대여량이 높은 것을 알 수 있습니다. 그 다음으로 높은 주중은 바로 월요일입니다. 즉, 금요일과 월요일은 주중임에도 불구하고 어느정도 주말의 속성을 가지고 있다는 사실을 알 수 있습니다.\n",
    "  * 이번에는 주말을 살펴보겠습니다. 일요일을 보자면, 토요일에 비해 상대적으로 자전거 대여량이 낮다는 사실을 알 수 있습니다. 이는 추측컨데 월요일의 피로도를 고려해서 토요일에 비해 대외 활동을 덜 가지는 것으로 생각할 수 있습니다.\n",
    "  \n",
    "이 분석을 통해 알 수 있는 사실은, 요일(datetime-dayofweek)을 머신러닝 알고리즘에 feature로 집어넣으면 근무일(workingday)만 집어넣는 것에 비해 더 좋은 성능을 낼 수 있다고 볼 수 있습니다. 그러므로 **요일(datetime-dayofweek)** 컬럼을 feature로 추가하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### count\n",
    "\n",
    "다음으로 분석할 컬럼은 **자전거 대여량(count)**입니다. 자전거 대여량(count)은 최종적으로 우리가 맞춰야 할 label이기 때문에, 다른 컬럼보다도 이 컬럼을 완벽하게 이해하는 것이 중요합니다. 먼저 seaborn의 [distplot](https://seaborn.pydata.org/generated/seaborn.distplot.html)으로 이 컬럼을 시각화 해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f390203ebe0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEKCAYAAAASByJ7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3905068d30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 자전거 대여량(count)의 분포를 시각화합니다.\n",
    "sns.distplot(train[\"count\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 그림에서 알 수 있는 내용은 다음과 같습니다.\n",
    "\n",
    "  * 자전거 대여량이 1 ~ 20대인 비중이 굉장히 높습니다.\n",
    "  * 반면에 자전거 대여량이 1,000대에 근접하는 경우도 있습니다. (977대)\n",
    "  \n",
    "위 두 개의 특성이 데이터를 왜곡되게(skewed) 만드는 것 같습니다. 이러한 경우 [log transformation](http://onlinestatbook.com/2/transformations/log.html)을 시도해 볼 만 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10886, 24)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>log_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>2.833213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40</td>\n",
       "      <td>3.713572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32</td>\n",
       "      <td>3.496508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>2.639057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count  log_count\n",
       "0     16   2.833213\n",
       "1     40   3.713572\n",
       "2     32   3.496508\n",
       "3     13   2.639057\n",
       "4      1   0.693147"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numpy라는 패키지를 불러옵니다.\n",
    "# 이 패키지는 선형대수(linear algebra) 패키지라고 불리는데,\n",
    "# 현재는 간단하게 '수학 연산을 편하게 해주는 패키지'라고 이해하시면 됩니다.\n",
    "import numpy as np\n",
    "\n",
    "# 자전거 대여량(count)에 +1을 한 후 log를 적용합니다.\n",
    "# 이를 log transformation이라고 합니다.\n",
    "train[\"log_count\"] = np.log(train[\"count\"] + 1)\n",
    "\n",
    "# train 변수에 할당된 데이터의 행렬 사이즈를 출력합니다.\n",
    "# 출력은 (row, column) 으로 표시됩니다.\n",
    "print(train.shape)\n",
    "\n",
    "# .head()로 train 데이터의 상위 5개를 띄우되,\n",
    "# count와 log_count 컬럼만 출력합니다.\n",
    "train[[\"count\", \"log_count\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f3902481b00>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABCEAAAELCAYAAADnfZwJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niIiIRISSECIiIiIiIiISEf8fpa7dzO4RC3QAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f390297d630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# matplotlib의 subplots를 사용합니다. 이 함수는 여러 개의 시각화를 한 화면에 띄울 수 있도록 합니다.\n",
    "# 이번에는 1x2로 총 2개의 시각화를 한 화면에 띄웁니다.\n",
    "figure, (ax1, ax2) = plt.subplots(nrows=1, ncols=2)\n",
    "\n",
    "# 시각화의 전체 사이즈는 18x4로 설정합니다.\n",
    "figure.set_size_inches(18, 4)\n",
    "\n",
    "# 좌측에는 자전거 대여량(count)의 분포를 시각화합니다.\n",
    "sns.distplot(train[\"count\"], ax=ax1)\n",
    "\n",
    "# 우측에는 log transformation한 자전거 대여량(log_count)의 분포를 시각화합니다.\n",
    "sns.distplot(train[\"log_count\"], ax=ax2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "비교 결과 좌측보다 우측이 훨씬 더 자연스럽습니다. (=[정규 분포](https://ko.wikipedia.org/wiki/%EC%A0%95%EA%B7%9C_%EB%B6%84%ED%8F%AC)에 가깝게 나옵니다.) 그 즉슨, **자전거 대여량(count)을 그대로 사용하는 것 보다, 이를 log transformation한 버전(log_count)을 사용하는게 더 좋은 정확도를 낼 수 있다고 가정할 수 있습니다.**\n",
    "\n",
    "다만 이 경우, 캐글에 제출하기 위해서는 log transformation한 버전을 원상복귀 시켜줘야 합니다. 이 경우에는 [exp](https://en.wikipedia.org/wiki/Exponential_function)를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10886, 25)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>log_count</th>\n",
       "      <th>count(recover)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>2.833213</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40</td>\n",
       "      <td>3.713572</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32</td>\n",
       "      <td>3.496508</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>2.639057</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count  log_count  count(recover)\n",
       "0     16   2.833213            16.0\n",
       "1     40   3.713572            40.0\n",
       "2     32   3.496508            32.0\n",
       "3     13   2.639057            13.0\n",
       "4      1   0.693147             1.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# log transformation한 자전거 대여량(log_count)을 다시 exp로 원상복귀 합니다.\n",
    "# (=자연로그는 exp로 없애버릴 수 있기 때문입니다)\n",
    "# 이를 count(recover)라는 새로운 컬럼에 대입합니다.\n",
    "train[\"count(recover)\"] = np.exp(train[\"log_count\"]) - 1\n",
    "\n",
    "# train 변수에 할당된 데이터의 행렬 사이즈를 출력합니다.\n",
    "# 출력은 (row, column) 으로 표시됩니다.\n",
    "print(train.shape)\n",
    "\n",
    "# .head()로 train 데이터의 상위 5개를 띄우되,\n",
    "# count, log_count, 그리고 count(recover) 컬럼만 출력합니다.\n",
    "train[[\"count\", \"log_count\", \"count(recover)\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f3901b70940>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABCEAAAELCAYAAADnfZwJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOCSXUQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3902872fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# matplotlib의 subplots를 사용합니다. 이 함수는 여러 개의 시각화를 한 화면에 띄울 수 있도록 합니다.\n",
    "# 이번에는 1x3로 총 3개의 시각화를 한 화면에 띄웁니다.\n",
    "figure, (ax1, ax2, ax3) = plt.subplots(nrows=1, ncols=3)\n",
    "\n",
    "# 시각화의 전체 사이즈는 18x4로 설정합니다.\n",
    "figure.set_size_inches(18, 4)\n",
    "\n",
    "# 좌측에는 자전거 대여량(count)의 분포를 시각화합니다.\n",
    "sns.distplot(train[\"count\"], ax=ax1)\n",
    "\n",
    "# 가운데에는 log transformation한 자전거 대여량(log_count)의 분포를 시각화합니다.\n",
    "sns.distplot(train[\"log_count\"], ax=ax2)\n",
    "\n",
    "# 우측에는 log transformation을 다시 원상복귀한 버전(count(recover))의 분포를 시각화합니다.\n",
    "sns.distplot(train[\"count(recover)\"], ax=ax3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "이제 분석을 통해 발견한 인사이트를 활용해보겠습니다.\n",
    "\n",
    "이전 경진대회와 마찬가지로, 이번에도 머신러닝 알고리즘을 사용하겠습니다. 이번에도 변함없이 [지도학습(Supervised Learning)](http://solarisailab.com/archives/1785) 알고리즘을 사용할 계획이기 때문에, 데이터를 Label(맞춰야 하는 정답)과 Feature(Label을 맞추는데 도움이 되는 값들)로 나눌 필요가 있습니다.\n",
    "\n",
    "이번 경진대회에서는 다음의 컬럼들을 Feature와 Label로 활용할 것입니다.\n",
    "\n",
    "  * **Feature**: 1) 계절(season), 2) 공휴일(holiday), 3) 근무일(workingday), 4) 날씨(weather), 5) 온도(temp), 6) 체감 온도(atemp), 7) 습도(humidity), 8) 풍속(weather), 9) 연도(datetime-year), 10) 시간(datetime-hour), 마지막으로 11) 요일(datetime-dayofweek) 입니다.\n",
    "  \n",
    "  * **Label**: log transformation한 자전거 대여량(log_count)을 사용합니다.\n",
    "  \n",
    "이를 통해 train 데이터와 test 데이터를 다음의 세 가지 형태의 값으로 나눌 것입니다.\n",
    "\n",
    "  * **X_train**: train 데이터의 feature 입니다. 줄여서 X_train이라고 부릅니다.\n",
    "  * **X_test**: test 데이터의 feature 입니다. 마찬가지로 줄여서 X_test라고 부릅니다.\n",
    "  * **y_train**: train 데이터의 label 입니다. 마찬가지로 줄여서 y_train이라고 부릅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['season',\n",
       " 'holiday',\n",
       " 'workingday',\n",
       " 'weather',\n",
       " 'temp',\n",
       " 'atemp',\n",
       " 'humidity',\n",
       " 'windspeed',\n",
       " 'datetime-year',\n",
       " 'datetime-hour',\n",
       " 'datetime-dayofweek']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 총 11개의 컬럼을 feature를 지정합니다.\n",
    "# 이 11개의 컬럼명을 feature_names라는 이름의 파이썬 리스트(list)로 만들어 변수에 할당합니다.\n",
    "feature_names = [\"season\", \"holiday\", \"workingday\", \"weather\",\n",
    "                 \"temp\", \"atemp\", \"humidity\", \"windspeed\",\n",
    "                 \"datetime-year\", \"datetime-hour\", \"datetime-dayofweek\"]\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'log_count'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# log transformation한 자전거 대여량(log_count)을 label로 지정합니다.\n",
    "label_name = \"log_count\"\n",
    "label_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10886, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season</th>\n",
       "      <th>holiday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weather</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>humidity</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>datetime-year</th>\n",
       "      <th>datetime-hour</th>\n",
       "      <th>datetime-dayofweek</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.84</td>\n",
       "      <td>14.395</td>\n",
       "      <td>81</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2011</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.02</td>\n",
       "      <td>13.635</td>\n",
       "      <td>80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.02</td>\n",
       "      <td>13.635</td>\n",
       "      <td>80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2011</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.84</td>\n",
       "      <td>14.395</td>\n",
       "      <td>75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2011</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.84</td>\n",
       "      <td>14.395</td>\n",
       "      <td>75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2011</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   season  holiday  workingday  weather  temp   atemp  humidity  windspeed  \\\n",
       "0       1        0           0        1  9.84  14.395        81        0.0   \n",
       "1       1        0           0        1  9.02  13.635        80        0.0   \n",
       "2       1        0           0        1  9.02  13.635        80        0.0   \n",
       "3       1        0           0        1  9.84  14.395        75        0.0   \n",
       "4       1        0           0        1  9.84  14.395        75        0.0   \n",
       "\n",
       "   datetime-year  datetime-hour  datetime-dayofweek  \n",
       "0           2011              0                   5  \n",
       "1           2011              1                   5  \n",
       "2           2011              2                   5  \n",
       "3           2011              3                   5  \n",
       "4           2011              4                   5  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature_names를 활용해 train 데이터의 feature를 가져옵니다.\n",
    "# 이를 X_train이라는 이름의 변수에 할당합니다.\n",
    "X_train = train[feature_names]\n",
    "\n",
    "# X_train 변수에 할당된 데이터의 행렬 사이즈를 출력합니다.\n",
    "# 출력은 (row, column) 으로 표시됩니다.\n",
    "print(X_train.shape)\n",
    "\n",
    "# X_train 데이터의 상위 5개를 띄웁니다.\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6493, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season</th>\n",
       "      <th>holiday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weather</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>humidity</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>datetime-year</th>\n",
       "      <th>datetime-hour</th>\n",
       "      <th>datetime-dayofweek</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10.66</td>\n",
       "      <td>11.365</td>\n",
       "      <td>56</td>\n",
       "      <td>26.0027</td>\n",
       "      <td>2011</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10.66</td>\n",
       "      <td>13.635</td>\n",
       "      <td>56</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10.66</td>\n",
       "      <td>13.635</td>\n",
       "      <td>56</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2011</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10.66</td>\n",
       "      <td>12.880</td>\n",
       "      <td>56</td>\n",
       "      <td>11.0014</td>\n",
       "      <td>2011</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10.66</td>\n",
       "      <td>12.880</td>\n",
       "      <td>56</td>\n",
       "      <td>11.0014</td>\n",
       "      <td>2011</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   season  holiday  workingday  weather   temp   atemp  humidity  windspeed  \\\n",
       "0       1        0           1        1  10.66  11.365        56    26.0027   \n",
       "1       1        0           1        1  10.66  13.635        56     0.0000   \n",
       "2       1        0           1        1  10.66  13.635        56     0.0000   \n",
       "3       1        0           1        1  10.66  12.880        56    11.0014   \n",
       "4       1        0           1        1  10.66  12.880        56    11.0014   \n",
       "\n",
       "   datetime-year  datetime-hour  datetime-dayofweek  \n",
       "0           2011              0                   3  \n",
       "1           2011              1                   3  \n",
       "2           2011              2                   3  \n",
       "3           2011              3                   3  \n",
       "4           2011              4                   3  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature_names를 활용해 test 데이터의 feature를 가져옵니다.\n",
    "# 이를 X_test라는 이름의 변수에 할당합니다.\n",
    "X_test = test[feature_names]\n",
    "\n",
    "# X_test 변수에 할당된 데이터의 행렬 사이즈를 출력합니다.\n",
    "# 출력은 (row, column) 으로 표시됩니다.\n",
    "print(X_test.shape)\n",
    "\n",
    "# X_test 데이터의 상위 5개를 띄웁니다.\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10886,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    2.833213\n",
       "1    3.713572\n",
       "2    3.496508\n",
       "3    2.639057\n",
       "4    0.693147\n",
       "Name: log_count, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# label_name을 활용해 train 데이터의 label을 가져옵니다.\n",
    "# 이를 y_train이라는 이름의 변수에 할당합니다.\n",
    "y_train = train[label_name]\n",
    "\n",
    "# y_train 변수에 할당된 데이터의 사이즈를 출력합니다.\n",
    "# 출력은 (row, column) 으로 표시되나, column이 없기 때문에 (row,) 형태로 표시될 것입니다.\n",
    "print(y_train.shape)\n",
    "\n",
    "# y_train 데이터의 상위 5개를 띄웁니다.\n",
    "y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate\n",
    "\n",
    "머신러닝 모델을 학습시키기 전에, 측정 공식(Evaluation Metric)을 통해 학습한 모델의 성능이 얼마나 뛰어난지 정량적으로 측정해보겠습니다. 이번 [Bike Sharing Demand](https://www.kaggle.com/c/bike-sharing-demand) 경진대회에서 사용하는 측정 공식은 Root Mean Squared Logarithmic Error ([RMSLE](https://www.kaggle.com/c/bike-sharing-demand#evaluation)) 입니다.\n",
    "\n",
    "$$ \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (\\log(p_i + 1) - \\log(a_i+1))^2 } $$\n",
    "\n",
    "이 공식은 정답($a_i$, actual)과 예측값($p_i$, predict)의 차이가 크면 클수록 **페널티를 덜 주는** 방식으로 동작합니다. (```log(count + 1)```이 그 역할을 합니다)\n",
    "\n",
    "다만 현재 이미 log transformation한 count(log_count)을 사용하고 있기 때문에, 이 공식을 그대로 사용할 경우 **사실상 ```log(count + 1)```를 두 번 하게 되는 셈이 됩니다.** 이를 방지하기 위해, 측정 공식에서 ```log(count + 1)```을 제거하도록 하겠습니다.\n",
    "\n",
    "$$ \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (p_i - a_i)^2 } $$\n",
    "\n",
    "이 공식을 Root Mean Squared Error ([RMSE](https://en.wikipedia.org/wiki/Root-mean-square_deviation))라고 합니다. 파이썬과 [numpy](http://www.numpy.org/), [scikit-learn](http://scikit-learn.org/stable/)으로 **RMSE** 공식을 구현해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "make_scorer(rmse)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numpy라는 패키지를 불러옵니다.\n",
    "# 이 패키지는 선형대수(linear algebra) 패키지라고 불리는데,\n",
    "# 현재는 간단하게 '수학 연산을 편하게 해주는 패키지'라고 이해하시면 됩니다.\n",
    "import numpy as np\n",
    "\n",
    "# scikit-learn 패키지의 metrics 모듈에서 make_scorer라는 함수를 가지고 옵니다.\n",
    "# 이 함수는 파이썬을 구현한 측정 공식을 scikit-learn에서 사용할 수 있도록 변환해 줍니다.\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# RMSE 공식을 구현한 함수를 생성합니다.\n",
    "# 이 함수는 예측값(predict)과 정답(actual)을 인자로 받습니다.\n",
    "def rmse(predict, actual):\n",
    "    # predict와 actual을 numpy array로 변환합니다.\n",
    "    # 이렇게 하면 수학 연산을 편하게 할 수 있습니다.\n",
    "    predict = np.array(predict)\n",
    "    actual = np.array(actual)\n",
    "\n",
    "    # 공식에 쓰여진대로 predict와 actual을 빼서 차이를 구합니다.\n",
    "    # 이 차이를 distance라는 이름의 새로운 변수에 할당합니다.\n",
    "    distance = predict - actual\n",
    "    \n",
    "    # 공식에 쓰여진대로 distance를 제곱합니다.\n",
    "    # 이 결과를 square_distance라는 이름의 새로운 변수에 할당합니다.\n",
    "    square_distance = distance ** 2\n",
    "    \n",
    "    # 공식에 쓰여진대로 square_distance의 평균을 구합니다.\n",
    "    # 이 결과를 mean_square_distance라는 이름의 새로운 변수에 할당합니다.\n",
    "    mean_square_distance = square_distance.mean()\n",
    "    \n",
    "    # 공식에 쓰여진대로 mean_square_distance에 루트(sqrt)를 씌웁니다.\n",
    "    # 이 결과를 score라는 이름의 새로운 변수에 할당합니다.\n",
    "    score = np.sqrt(mean_square_distance)\n",
    "    \n",
    "    # score 변수를 반환합니다.\n",
    "    return score\n",
    "\n",
    "# scikit-learn의 make_scorer를 활용하여\n",
    "# rmse 함수를 scikit-learn의 다른 함수에서 사용할 수 있도록 변환합니다.\n",
    "# 이 결과를 rmse_score라는 이름의 새로운 변수에 할당합니다.\n",
    "rmse_score = make_scorer(rmse)\n",
    "rmse_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "이번에는 머신러닝 모델의 하이퍼패러미터를 튜닝해보겠습니다.\n",
    "\n",
    "머신러닝 모델에는 다양한 옵션이 있는데, 이 옵션을 통해 모델의 성능을 끌어올릴 수 있습니다. 이 옵션들을 전문용어로 하이퍼패러미터(Hyperparameter)라고 부릅니다. 만일 적절한 하이퍼패러미터를 찾아서 모델에 적용할 수 있다면 모델의 성능을 한 층 더 끌어올릴 수 있습니다. 이를 **하이퍼패러미터 튜닝(Hyperparamter Tuning)**이라고 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, colsample_bylevel=1, colsample_bytree=1, gamma=0,\n",
       "       learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
       "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
       "       objective='reg:linear', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=0, silent=True, subsample=1)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradient Boosting Machine 패키지인 XGBoost를 가져옵니다.\n",
    "# 이를 xgb라는 축약어로 사용합니다.\n",
    "import xgboost as xgb\n",
    "\n",
    "# XGBRegressor를 생성합니다.\n",
    "# 생성한 모델을 출력하면 다양한 하이퍼패러미터(n_estimators, max_depth, etc)들이 있는 것을 확인할 수 있습니다.\n",
    "xgb.XGBRegressor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하이어패러미터를 튜닝하는 방법은 크게 두 가지가 있습니다. 첫 번째로 Grid Search라는 방식이고, 두 번째는 Coarse & Finer Search라는 방식입니다. 먼저 Grid Search부터 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 1 - Grid Search\n",
    "\n",
    "Grid Search는 몇 개의 하이퍼패리미터 후보군을 정한 뒤 이를 계속 조합해가며 가장 좋은 하이퍼패리미터를 찾는 방식입니다. Cross Validation 점수가 가장 좋은 하이퍼패러미터를 가장 좋은 하이퍼패러미터라고 간주합니다. (자세한 사항은 [다음의 링크](http://scikit-learn.org/stable/modules/grid_search.html)를 참고 바랍니다)\n",
    "\n",
    "보통은 [scikit-learn](http://scikit-learn.org/)의 [GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)를 사용합니다만, 이번에는 Grid Search를 더 자세히 이해하기 위해 이를 직접 구현해보겠습니다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators = 100, max_depth = 50, learning_rate = 1.000000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 0.88140\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 1.000000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 1.04028\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 1.000000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 0.74037\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 1.000000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 1.22020\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 1.000000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 0.99091\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 1.000000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 0.68523\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 1.000000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 1.60839\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 1.000000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 0.73196\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 1.000000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 0.66976\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 1.000000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 0.61069\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 1.000000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 0.75889\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 1.000000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 0.58929\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 1.000000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 0.86022\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 1.000000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 0.61284\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 1.000000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 0.53668\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 1.000000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 1.18265\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 1.000000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 0.58497\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 1.000000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 0.51850\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 1.000000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 0.53155\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 1.000000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 0.70665\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 1.000000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 0.51485\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 1.000000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 0.84865\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 1.000000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 0.64217\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 1.000000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 0.47024\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 1.000000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 1.24395\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 1.000000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 0.66669\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 1.000000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 0.47178\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.100000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 0.53103\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.100000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 0.38397\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.100000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 0.35315\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.100000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 0.42191\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.100000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 0.37249\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.100000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 0.34473\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.100000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 0.52158\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.100000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 0.36707\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.100000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 0.34185\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.100000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 0.51322\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.100000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 0.38583\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.100000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 0.34562\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.100000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 0.41997\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.100000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 0.36695\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.100000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 0.34069\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.100000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 0.53935\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.100000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 0.37323\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.100000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 0.34397\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.100000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 0.52874\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.100000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 0.37509\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.100000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 0.34661\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.100000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 0.43909\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.100000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 0.37413\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.100000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 0.34096\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.100000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 0.47844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators = 100, max_depth = 50, learning_rate = 0.100000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 0.41386\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.100000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 0.35398\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.010000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 1.88250\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.010000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 1.76747\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.010000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 1.67506\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.010000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 1.82334\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.010000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 1.72101\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.010000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 1.65721\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.010000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 1.80464\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.010000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 1.70569\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.010000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 1.65600\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.010000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 1.87677\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.010000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 1.75870\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.010000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 1.67053\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.010000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 1.80198\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.010000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 1.71315\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.010000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 1.65303\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.010000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 1.80207\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.010000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 1.70080\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.010000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 1.64972\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.010000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 1.88526\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.010000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 1.75469\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.010000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 1.66419\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.010000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 1.81615\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.010000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 1.70964\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.010000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 1.64731\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.010000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 1.75960\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.010000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 1.69044\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.010000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 1.64830\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.001000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 3.93626\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.001000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 3.92659\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.001000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 3.91260\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.001000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 3.93155\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.001000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 3.92023\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.001000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 3.90985\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.001000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 3.92866\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.001000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 3.91826\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.001000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 3.90992\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.001000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 3.93510\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.001000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 3.92519\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.001000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 3.91216\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.001000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 3.92912\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.001000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 3.91897\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.001000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 3.90897\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.001000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 3.92834\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.001000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 3.91773\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.001000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 3.90893\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.001000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 3.93433\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.001000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 3.92385\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.001000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 3.91177\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.001000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 3.92982\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.001000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 3.91809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators = 100, max_depth = 50, learning_rate = 0.001000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 3.90866\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.001000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 3.92437\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.001000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 3.91639\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.001000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 3.90895\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.000100, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 4.27128\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.000100, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 4.27038\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.000100, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 4.26896\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.000100, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 4.27075\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.000100, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 4.26977\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.000100, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 4.26872\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.000100, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 4.27062\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.000100, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 4.26957\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.000100, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 4.26869\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.000100, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 4.27120\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.000100, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 4.27032\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.000100, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 4.26889\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.000100, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 4.27060\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.000100, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 4.26964\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.000100, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 4.26858\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.000100, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 4.27058\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.000100, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 4.26951\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.000100, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 4.26859\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.000100, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 4.27119\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.000100, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 4.27003\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.000100, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 4.26886\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.000100, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 4.27073\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.000100, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 4.26965\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.000100, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 4.26857\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.000100, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 4.27019\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.000100, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 4.26937\n",
      "n_estimators = 100, max_depth = 50, learning_rate = 0.000100, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 4.26854\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 1.000000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 0.88140\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 1.000000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 1.04028\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 1.000000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 0.74037\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 1.000000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 1.22020\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 1.000000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 0.99091\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 1.000000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 0.68523\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 1.000000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 1.60839\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 1.000000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 0.73196\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 1.000000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 0.66976\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 1.000000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 0.61069\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 1.000000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 0.75889\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 1.000000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 0.58929\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 1.000000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 0.86022\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 1.000000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 0.61284\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 1.000000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 0.53668\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 1.000000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 1.18265\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 1.000000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 0.58497\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 1.000000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 0.51850\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 1.000000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 0.53155\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 1.000000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 0.70665\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 1.000000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 0.51483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators = 100, max_depth = 75, learning_rate = 1.000000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 0.84865\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 1.000000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 0.64217\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 1.000000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 0.47010\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 1.000000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 1.24395\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 1.000000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 0.66669\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 1.000000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 0.47178\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.100000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 0.53103\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.100000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 0.38397\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.100000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 0.35315\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.100000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 0.42191\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.100000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 0.37249\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.100000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 0.34473\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.100000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 0.52158\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.100000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 0.36707\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.100000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 0.34185\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.100000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 0.51322\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.100000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 0.38583\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.100000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 0.34562\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.100000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 0.41997\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.100000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 0.36695\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.100000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 0.34069\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.100000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 0.53935\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.100000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 0.37323\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.100000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 0.34397\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.100000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 0.52874\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.100000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 0.37509\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.100000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 0.34661\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.100000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 0.43909\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.100000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 0.37413\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.100000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 0.34096\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.100000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 0.47844\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.100000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 0.41386\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.100000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 0.35398\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.010000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 1.88250\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.010000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 1.76747\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.010000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 1.67506\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.010000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 1.82334\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.010000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 1.72101\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.010000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 1.65721\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.010000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 1.80464\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.010000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 1.70569\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.010000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 1.65600\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.010000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 1.87677\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.010000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 1.75870\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.010000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 1.67053\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.010000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 1.80198\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.010000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 1.71315\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.010000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 1.65303\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.010000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 1.80207\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.010000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 1.70080\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.010000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 1.64972\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.010000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 1.88526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators = 100, max_depth = 75, learning_rate = 0.010000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 1.75469\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.010000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 1.66419\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.010000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 1.81615\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.010000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 1.70964\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.010000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 1.64731\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.010000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 1.75960\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.010000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 1.69044\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.010000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 1.64830\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.001000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 3.93626\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.001000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 3.92659\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.001000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 3.91260\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.001000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 3.93155\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.001000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 3.92023\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.001000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 3.90985\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.001000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 3.92866\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.001000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 3.91826\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.001000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 3.90992\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.001000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 3.93510\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.001000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 3.92519\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.001000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 3.91216\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.001000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 3.92912\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.001000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 3.91897\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.001000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 3.90897\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.001000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 3.92834\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.001000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 3.91773\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.001000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 3.90893\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.001000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 3.93433\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.001000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 3.92385\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.001000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 3.91177\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.001000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 3.92982\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.001000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 3.91809\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.001000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 3.90866\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.001000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 3.92437\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.001000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 3.91639\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.001000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 3.90895\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.000100, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 4.27128\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.000100, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 4.27038\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.000100, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 4.26896\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.000100, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 4.27075\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.000100, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 4.26977\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.000100, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 4.26872\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.000100, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 4.27062\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.000100, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 4.26957\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.000100, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 4.26869\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.000100, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 4.27120\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.000100, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 4.27032\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.000100, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 4.26889\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.000100, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 4.27060\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.000100, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 4.26964\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.000100, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 4.26858\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.000100, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 4.27058\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.000100, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 4.26951\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators = 100, max_depth = 75, learning_rate = 0.000100, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 4.26859\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.000100, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 4.27119\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.000100, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 4.27003\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.000100, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 4.26886\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.000100, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 4.27073\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.000100, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 4.26965\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.000100, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 4.26857\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.000100, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 4.27019\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.000100, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 4.26937\n",
      "n_estimators = 100, max_depth = 75, learning_rate = 0.000100, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 4.26854\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 1.000000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 0.88140\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 1.000000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 1.04028\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 1.000000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 0.74037\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 1.000000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 1.22020\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 1.000000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 0.99091\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 1.000000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 0.68523\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 1.000000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 1.60839\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 1.000000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 0.73196\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 1.000000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 0.66976\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 1.000000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 0.61069\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 1.000000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 0.75889\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 1.000000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 0.58929\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 1.000000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 0.86022\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 1.000000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 0.61284\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 1.000000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 0.53668\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 1.000000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 1.18265\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 1.000000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 0.58497\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 1.000000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 0.51850\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 1.000000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 0.53155\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 1.000000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 0.70665\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 1.000000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 0.51483\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 1.000000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 0.84865\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 1.000000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 0.64217\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 1.000000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 0.47010\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 1.000000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 1.24395\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 1.000000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 0.66669\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 1.000000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 0.47178\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.100000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 0.53103\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.100000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 0.38397\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.100000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 0.35315\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.100000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 0.42191\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.100000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 0.37249\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.100000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 0.34473\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.100000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 0.52158\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.100000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 0.36707\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.100000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 0.34185\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.100000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 0.51322\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.100000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 0.38583\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.100000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 0.34562\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.100000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 0.41997\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.100000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 0.36695\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.100000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 0.34069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators = 100, max_depth = 100, learning_rate = 0.100000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 0.53935\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.100000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 0.37323\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.100000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 0.34397\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.100000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 0.52874\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.100000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 0.37509\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.100000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 0.34661\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.100000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 0.43909\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.100000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 0.37413\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.100000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 0.34096\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.100000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 0.47844\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.100000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 0.41386\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.100000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 0.35398\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.010000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 1.88250\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.010000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 1.76747\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.010000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 1.67506\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.010000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 1.82334\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.010000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 1.72101\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.010000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 1.65721\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.010000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 1.80464\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.010000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 1.70569\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.010000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 1.65600\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.010000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 1.87677\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.010000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 1.75870\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.010000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 1.67053\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.010000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 1.80198\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.010000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 1.71315\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.010000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 1.65303\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.010000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 1.80207\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.010000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 1.70080\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.010000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 1.64972\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.010000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 1.88526\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.010000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 1.75469\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.010000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 1.66419\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.010000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 1.81615\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.010000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 1.70964\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.010000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 1.64731\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.010000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 1.75960\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.010000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 1.69044\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.010000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 1.64830\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.001000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 3.93626\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.001000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 3.92659\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.001000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 3.91260\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.001000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 3.93155\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.001000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 3.92023\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.001000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 3.90985\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.001000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 3.92866\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.001000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 3.91826\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.001000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 3.90992\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.001000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 3.93510\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.001000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 3.92519\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.001000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 3.91216\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.001000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 3.92912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators = 100, max_depth = 100, learning_rate = 0.001000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 3.91897\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.001000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 3.90897\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.001000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 3.92834\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.001000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 3.91773\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.001000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 3.90893\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.001000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 3.93433\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.001000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 3.92385\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.001000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 3.91177\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.001000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 3.92982\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.001000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 3.91809\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.001000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 3.90866\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.001000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 3.92437\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.001000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 3.91639\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.001000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 3.90895\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.000100, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 4.27128\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.000100, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 4.27038\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.000100, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 4.26896\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.000100, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 4.27075\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.000100, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 4.26977\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.000100, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 4.26872\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.000100, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 4.27062\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.000100, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 4.26957\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.000100, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 4.26869\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.000100, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 4.27120\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.000100, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 4.27032\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.000100, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 4.26889\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.000100, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 4.27060\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.000100, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 4.26964\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.000100, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 4.26858\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.000100, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 4.27058\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.000100, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 4.26951\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.000100, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 4.26859\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.000100, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 4.27119\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.000100, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 4.27003\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.000100, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 4.26886\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.000100, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 4.27073\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.000100, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 4.26965\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.000100, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 4.26857\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.000100, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 4.27019\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.000100, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 4.26937\n",
      "n_estimators = 100, max_depth = 100, learning_rate = 0.000100, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 4.26854\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 1.000000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 1.03727\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 1.000000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 1.04561\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 1.000000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 0.74806\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 1.000000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 1.25190\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 1.000000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 0.99481\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 1.000000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 0.69283\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 1.000000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 1.61467\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 1.000000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 0.73941\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 1.000000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 0.67267\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 1.000000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 0.64444\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 1.000000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 0.76162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators = 300, max_depth = 50, learning_rate = 1.000000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 0.59338\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 1.000000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 0.86105\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 1.000000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 0.61517\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 1.000000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 0.53912\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 1.000000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 1.18252\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 1.000000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 0.58913\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 1.000000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 0.52103\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 1.000000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 0.53749\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 1.000000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 0.70665\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 1.000000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 0.51485\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 1.000000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 0.84864\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 1.000000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 0.64217\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 1.000000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 0.47024\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 1.000000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 1.24395\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 1.000000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 0.66669\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 1.000000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 0.47178\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.100000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 0.39132\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.100000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 0.37640\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.100000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 0.35418\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.100000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 0.38960\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.100000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 0.37248\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.100000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 0.34631\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.100000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 0.48697\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.100000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 0.36704\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.100000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 0.34348\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.100000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 0.39122\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.100000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 0.37948\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.100000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 0.34587\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.100000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 0.39368\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.100000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 0.36662\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.100000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 0.34112\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.100000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 0.50976\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.100000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 0.37297\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.100000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 0.34438\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.100000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 0.38995\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.100000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 0.37092\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.100000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 0.34658\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.100000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 0.40778\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.100000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 0.37358\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.100000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 0.34093\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.100000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 0.46860\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.100000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 0.41328\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.100000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 0.35397\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.010000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 0.87125\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.010000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 0.58177\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.010000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 0.44646\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.010000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 0.68789\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.010000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 0.49692\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.010000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 0.42465\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.010000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 0.66506\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.010000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 0.48266\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.010000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 0.42327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators = 300, max_depth = 50, learning_rate = 0.010000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 0.86018\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.010000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 0.57311\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.010000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 0.44183\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.010000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 0.68430\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.010000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 0.48656\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.010000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 0.42404\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.010000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 0.66445\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.010000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 0.48279\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.010000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 0.42419\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.010000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 0.87578\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.010000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 0.56675\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.010000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 0.43985\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.010000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 0.69381\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.010000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 0.48766\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.010000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 0.42366\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.010000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 0.66380\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.010000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 0.48582\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.010000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 0.43369\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.001000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 3.29585\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.001000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 3.26528\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.001000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 3.22829\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.001000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 3.27742\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.001000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 3.24721\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.001000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 3.22036\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.001000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 3.27241\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.001000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 3.24596\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.001000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 3.22021\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.001000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 3.29594\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.001000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 3.26192\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.001000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 3.22646\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.001000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 3.27474\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.001000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 3.24555\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.001000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 3.21784\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.001000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 3.27122\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.001000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 3.24424\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.001000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 3.21876\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.001000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 3.29446\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.001000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 3.25987\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.001000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 3.22425\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.001000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 3.27360\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.001000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 3.24382\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.001000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 3.21686\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.001000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 3.26918\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.001000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 3.24314\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.001000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 3.21789\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.000100, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 4.19404\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.000100, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 4.19115\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.000100, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 4.18708\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.000100, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 4.19218\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.000100, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 4.18929\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.000100, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 4.18621\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.000100, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 4.19183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators = 300, max_depth = 50, learning_rate = 0.000100, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 4.18912\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.000100, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 4.18620\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.000100, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 4.19385\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.000100, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 4.19079\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.000100, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 4.18681\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.000100, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 4.19202\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.000100, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 4.18902\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.000100, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 4.18584\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.000100, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 4.19170\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.000100, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 4.18895\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.000100, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 4.18587\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.000100, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 4.19389\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.000100, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 4.19032\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.000100, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 4.18662\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.000100, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 4.19202\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.000100, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 4.18896\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.000100, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 4.18584\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.000100, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 4.19149\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.000100, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 4.18882\n",
      "n_estimators = 300, max_depth = 50, learning_rate = 0.000100, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 4.18563\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 1.000000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 1.03727\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 1.000000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 1.04561\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 1.000000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 0.74806\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 1.000000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 1.25190\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 1.000000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 0.99481\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 1.000000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 0.69283\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 1.000000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 1.61467\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 1.000000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 0.73941\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 1.000000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 0.67267\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 1.000000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 0.64444\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 1.000000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 0.76162\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 1.000000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 0.59338\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 1.000000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 0.86105\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 1.000000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 0.61517\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 1.000000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 0.53912\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 1.000000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 1.18252\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 1.000000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 0.58913\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 1.000000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 0.52103\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 1.000000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 0.53749\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 1.000000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 0.70665\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 1.000000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 0.51483\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 1.000000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 0.84864\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 1.000000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 0.64217\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 1.000000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 0.47010\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 1.000000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 1.24395\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 1.000000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 0.66669\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 1.000000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 0.47178\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.100000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 0.39132\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.100000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 0.37640\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.100000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 0.35418\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.100000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 0.38960\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.100000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 0.37248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators = 300, max_depth = 75, learning_rate = 0.100000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 0.34631\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.100000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 0.48697\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.100000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 0.36704\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.100000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 0.34343\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.100000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 0.39122\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.100000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 0.37948\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.100000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 0.34591\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.100000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 0.39368\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.100000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 0.36662\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.100000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 0.34112\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.100000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 0.50976\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.100000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 0.37297\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.100000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 0.34438\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.100000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 0.38995\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.100000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 0.37092\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.100000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 0.34658\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.100000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 0.40778\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.100000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 0.37358\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.100000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 0.34093\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.100000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 0.46860\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.100000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 0.41328\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.100000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 0.35397\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.010000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 0.87125\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.010000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 0.58177\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.010000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 0.44646\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.010000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 0.68789\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.010000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 0.49692\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.010000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 0.42465\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.010000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 0.66506\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.010000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 0.48266\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.010000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 0.42327\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.010000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 0.86018\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.010000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 0.57311\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.010000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 0.44183\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.010000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 0.68430\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.010000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 0.48656\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.010000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 0.42404\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.010000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 0.66445\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.010000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 0.48279\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.010000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 0.42419\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.010000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 0.87578\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.010000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 0.56675\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.010000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 0.43985\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.010000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 0.69381\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.010000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 0.48766\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.010000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 0.42366\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.010000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 0.66380\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.010000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 0.48582\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.010000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 0.43369\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.001000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 3.29585\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.001000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 3.26528\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.001000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 3.22829\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators = 300, max_depth = 75, learning_rate = 0.001000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 3.27742\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.001000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 3.24721\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.001000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 3.22036\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.001000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 3.27241\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.001000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 3.24596\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.001000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 3.22021\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.001000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 3.29594\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.001000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 3.26192\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.001000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 3.22646\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.001000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 3.27474\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.001000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 3.24555\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.001000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 3.21784\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.001000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 3.27122\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.001000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 3.24424\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.001000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 3.21876\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.001000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 3.29446\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.001000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 3.25987\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.001000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 3.22425\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.001000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 3.27360\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.001000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 3.24382\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.001000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 3.21686\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.001000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 3.26918\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.001000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 3.24314\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.001000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 3.21789\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.000100, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 4.19404\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.000100, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 4.19115\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.000100, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 4.18708\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.000100, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 4.19218\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.000100, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 4.18929\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.000100, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 4.18621\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.000100, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 4.19183\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.000100, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 4.18912\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.000100, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 4.18620\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.000100, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 4.19385\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.000100, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 4.19079\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.000100, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 4.18681\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.000100, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 4.19202\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.000100, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 4.18902\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.000100, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 4.18584\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.000100, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 4.19170\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.000100, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 4.18895\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.000100, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 4.18587\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.000100, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 4.19389\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.000100, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 4.19032\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.000100, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 4.18662\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.000100, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 4.19202\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.000100, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 4.18896\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.000100, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 4.18584\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.000100, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 4.19149\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.000100, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 4.18882\n",
      "n_estimators = 300, max_depth = 75, learning_rate = 0.000100, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 4.18563\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 1.000000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 1.03727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators = 300, max_depth = 100, learning_rate = 1.000000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 1.04561\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 1.000000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 0.74806\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 1.000000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 1.25190\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 1.000000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 0.99481\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 1.000000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 0.69283\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 1.000000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 1.61467\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 1.000000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 0.73941\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 1.000000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 0.67267\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 1.000000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 0.64444\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 1.000000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 0.76162\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 1.000000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 0.59338\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 1.000000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 0.86105\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 1.000000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 0.61517\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 1.000000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 0.53912\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 1.000000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 1.18252\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 1.000000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 0.58913\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 1.000000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 0.52103\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 1.000000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 0.53749\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 1.000000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 0.70665\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 1.000000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 0.51483\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 1.000000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 0.84864\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 1.000000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 0.64217\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 1.000000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 0.47010\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 1.000000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 1.24395\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 1.000000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 0.66669\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 1.000000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 0.47178\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.100000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 0.39132\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.100000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 0.37640\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.100000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 0.35418\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.100000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 0.38960\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.100000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 0.37248\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.100000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 0.34631\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.100000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 0.48697\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.100000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 0.36704\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.100000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 0.34343\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.100000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 0.39122\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.100000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 0.37948\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.100000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 0.34591\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.100000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 0.39368\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.100000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 0.36662\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.100000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 0.34112\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.100000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 0.50976\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.100000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 0.37297\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.100000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 0.34438\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.100000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 0.38995\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.100000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 0.37092\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.100000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 0.34658\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.100000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 0.40778\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.100000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 0.37358\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.100000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 0.34093\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.100000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 0.46860\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.100000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 0.41328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators = 300, max_depth = 100, learning_rate = 0.100000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 0.35397\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.010000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 0.87125\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.010000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 0.58177\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.010000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 0.44646\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.010000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 0.68789\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.010000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 0.49692\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.010000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 0.42465\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.010000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 0.66506\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.010000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 0.48266\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.010000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 0.42327\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.010000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 0.86018\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.010000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 0.57311\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.010000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 0.44183\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.010000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 0.68430\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.010000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 0.48656\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.010000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 0.42404\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.010000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 0.66445\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.010000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 0.48279\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.010000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 0.42419\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.010000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 0.87578\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.010000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 0.56675\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.010000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 0.43985\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.010000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 0.69381\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.010000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 0.48766\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.010000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 0.42366\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.010000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 0.66380\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.010000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 0.48582\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.010000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 0.43369\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.001000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 3.29585\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.001000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 3.26528\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.001000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 3.22829\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.001000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 3.27742\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.001000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 3.24721\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.001000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 3.22036\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.001000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 3.27241\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.001000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 3.24596\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.001000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 3.22021\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.001000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 3.29594\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.001000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 3.26192\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.001000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 3.22646\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.001000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 3.27474\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.001000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 3.24555\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.001000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 3.21784\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.001000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 3.27122\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.001000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 3.24424\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.001000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 3.21876\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.001000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 3.29446\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.001000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 3.25987\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.001000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 3.22425\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.001000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 3.27360\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.001000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 3.24382\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.001000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 3.21686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators = 300, max_depth = 100, learning_rate = 0.001000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 3.26918\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.001000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 3.24314\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.001000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 3.21789\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.000100, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 4.19404\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.000100, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 4.19115\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.000100, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 4.18708\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.000100, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 4.19218\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.000100, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 4.18929\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.000100, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 4.18621\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.000100, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 4.19183\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.000100, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 4.18912\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.000100, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 4.18620\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.000100, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 4.19385\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.000100, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 4.19079\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.000100, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 4.18681\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.000100, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 4.19202\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.000100, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 4.18902\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.000100, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 4.18584\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.000100, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 4.19170\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.000100, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 4.18895\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.000100, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 4.18587\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.000100, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 4.19389\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.000100, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 4.19032\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.000100, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 4.18662\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.000100, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 4.19202\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.000100, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 4.18896\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.000100, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 4.18584\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.000100, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 4.19149\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.000100, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 4.18882\n",
      "n_estimators = 300, max_depth = 100, learning_rate = 0.000100, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 4.18563\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 1.000000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 1.16183\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 1.000000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 1.05518\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 1.000000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 0.76923\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 1.000000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 1.25621\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 1.000000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 1.01160\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 1.000000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 0.71913\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 1.000000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 1.62710\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 1.000000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 0.76576\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 1.000000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 0.68825\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 1.000000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 0.65312\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 1.000000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 0.76759\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 1.000000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 0.60073\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 1.000000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 0.86349\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 1.000000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 0.62512\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 1.000000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 0.55728\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 1.000000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 1.18495\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 1.000000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 0.60435\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 1.000000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 0.52858\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 1.000000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 0.53854\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 1.000000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 0.70665\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 1.000000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 0.51485\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 1.000000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 0.84864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators = 1000, max_depth = 50, learning_rate = 1.000000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 0.64217\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 1.000000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 0.47024\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 1.000000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 1.24395\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 1.000000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 0.66669\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 1.000000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 0.47178\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.100000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 0.36401\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.100000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 0.37708\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.100000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 0.35434\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.100000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 0.39023\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.100000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 0.37288\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.100000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 0.34657\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.100000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 0.48435\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.100000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 0.36710\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.100000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 0.34360\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.100000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 0.36301\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.100000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 0.37960\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.100000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 0.34608\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.100000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 0.39399\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.100000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 0.36677\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.100000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 0.34133\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.100000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 0.50752\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.100000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 0.37281\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.100000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 0.34425\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.100000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 0.36316\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.100000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 0.37092\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.100000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 0.34658\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.100000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 0.40725\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.100000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 0.37358\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.100000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 0.34093\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.100000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 0.46806\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.100000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 0.41328\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.100000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 0.35397\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.010000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 0.50462\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.010000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 0.35701\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.010000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 0.33262\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.010000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 0.39581\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.010000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 0.34670\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.010000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 0.32932\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.010000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 0.41676\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.010000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 0.34589\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.010000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 0.33061\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.010000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 0.50284\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.010000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 0.36157\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.010000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 0.33473\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.010000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 0.40915\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.010000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 0.34892\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.010000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 0.33178\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.010000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 0.42889\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.010000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 0.35402\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.010000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 0.33556\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.010000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 0.50934\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.010000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 0.36659\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.010000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 0.33682\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.010000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 0.40981\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.010000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 0.35600\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.010000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 0.33555\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.010000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 0.44791\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.010000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 0.36241\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.010000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 0.35090\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.001000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 1.89961\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.001000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 1.77525\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.001000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 1.68269\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.001000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 1.81981\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.001000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 1.72476\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.001000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 1.66408\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.001000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 1.79626\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.001000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 1.71422\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.001000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 1.66129\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.001000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 1.89732\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.001000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 1.76453\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.001000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 1.67659\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.001000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 1.81464\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.001000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 1.72087\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.001000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 1.65783\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.001000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 1.79305\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.001000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 1.71003\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.001000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 1.65633\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.001000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 1.89328\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.001000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 1.75946\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.001000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 1.67169\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.001000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 1.81101\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.001000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 1.71611\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.001000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 1.65437\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.001000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 1.79683\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.001000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 1.70776\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.001000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 1.65542\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.000100, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 3.93577\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.000100, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 3.92584\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.000100, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 3.91312\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.000100, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 3.92957\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.000100, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 3.92058\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.000100, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 3.91013\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.000100, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 3.92743\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.000100, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 3.91882\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.000100, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 3.91013\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.000100, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 3.93530\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.000100, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 3.92501\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.000100, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 3.91208\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.000100, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 3.92932\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.000100, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 3.91902\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.000100, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 3.90919\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.000100, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 3.92702\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.000100, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 3.91826\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.000100, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 3.90927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.000100, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 3.93542\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.000100, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 3.92385\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.000100, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 3.91149\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.000100, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 3.92862\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.000100, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 3.91903\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.000100, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 3.90895\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.000100, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 3.92731\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.000100, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 3.91805\n",
      "n_estimators = 1000, max_depth = 50, learning_rate = 0.000100, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 3.90912\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 1.000000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 1.16183\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 1.000000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 1.05518\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 1.000000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 0.76923\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 1.000000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 1.25621\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 1.000000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 1.01160\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 1.000000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 0.71913\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 1.000000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 1.62710\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 1.000000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 0.76576\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 1.000000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 0.68825\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 1.000000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 0.65312\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 1.000000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 0.76759\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 1.000000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 0.60073\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 1.000000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 0.86349\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 1.000000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 0.62512\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 1.000000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 0.55728\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 1.000000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 1.18495\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 1.000000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 0.60435\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 1.000000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 0.52858\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 1.000000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 0.53854\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 1.000000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 0.70665\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 1.000000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 0.51483\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 1.000000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 0.84864\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 1.000000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 0.64217\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 1.000000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 0.47010\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 1.000000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 1.24395\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 1.000000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 0.66669\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 1.000000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 0.47178\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.100000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 0.36401\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.100000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 0.37708\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.100000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 0.35434\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.100000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 0.39023\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.100000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 0.37288\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.100000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 0.34657\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.100000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 0.48435\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.100000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 0.36710\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.100000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 0.34356\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.100000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 0.36301\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.100000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 0.37960\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.100000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 0.34623\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.100000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 0.39399\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.100000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 0.36677\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.100000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 0.34133\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.100000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 0.50752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.100000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 0.37281\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.100000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 0.34424\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.100000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 0.36316\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.100000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 0.37092\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.100000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 0.34658\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.100000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 0.40725\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.100000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 0.37358\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.100000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 0.34093\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.100000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 0.46806\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.100000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 0.41328\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.100000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 0.35397\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.010000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 0.50462\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.010000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 0.35701\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.010000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 0.33262\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.010000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 0.39581\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.010000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 0.34670\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.010000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 0.32934\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.010000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 0.41676\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.010000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 0.34589\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.010000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 0.33061\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.010000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 0.50284\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.010000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 0.36157\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.010000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 0.33473\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.010000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 0.40915\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.010000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 0.34892\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.010000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 0.33178\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.010000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 0.42889\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.010000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 0.35402\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.010000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 0.33556\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.010000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 0.50934\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.010000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 0.36659\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.010000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 0.33682\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.010000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 0.40981\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.010000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 0.35600\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.010000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 0.33555\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.010000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 0.44791\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.010000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 0.36241\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.010000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 0.35090\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.001000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 1.89961\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.001000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 1.77525\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.001000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 1.68269\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.001000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 1.81981\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.001000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 1.72476\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.001000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 1.66408\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.001000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 1.79626\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.001000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 1.71422\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.001000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 1.66129\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.001000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 1.89732\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.001000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 1.76453\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.001000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 1.67659\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.001000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 1.81464\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.001000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 1.72087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.001000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 1.65783\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.001000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 1.79305\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.001000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 1.71003\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.001000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 1.65633\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.001000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 1.89328\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.001000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 1.75946\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.001000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 1.67169\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.001000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 1.81101\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.001000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 1.71611\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.001000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 1.65437\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.001000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 1.79683\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.001000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 1.70776\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.001000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 1.65542\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.000100, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 3.93577\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.000100, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 3.92584\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.000100, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 3.91312\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.000100, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 3.92957\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.000100, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 3.92058\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.000100, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 3.91013\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.000100, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 3.92743\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.000100, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 3.91882\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.000100, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 3.91013\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.000100, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 3.93530\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.000100, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 3.92501\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.000100, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 3.91208\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.000100, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 3.92932\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.000100, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 3.91902\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.000100, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 3.90919\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.000100, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 3.92702\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.000100, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 3.91826\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.000100, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 3.90927\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.000100, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 3.93542\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.000100, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 3.92385\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.000100, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 3.91149\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.000100, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 3.92862\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.000100, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 3.91903\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.000100, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 3.90895\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.000100, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 3.92731\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.000100, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 3.91805\n",
      "n_estimators = 1000, max_depth = 75, learning_rate = 0.000100, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 3.90912\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 1.000000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 1.16183\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 1.000000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 1.05518\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 1.000000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 0.76923\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 1.000000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 1.25621\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 1.000000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 1.01160\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 1.000000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 0.71913\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 1.000000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 1.62710\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 1.000000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 0.76576\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 1.000000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 0.68825\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 1.000000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 0.65312\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 1.000000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 0.76759\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 1.000000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 0.60073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators = 1000, max_depth = 100, learning_rate = 1.000000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 0.86349\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 1.000000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 0.62512\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 1.000000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 0.55728\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 1.000000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 1.18495\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 1.000000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 0.60435\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 1.000000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 0.52858\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 1.000000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 0.53854\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 1.000000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 0.70665\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 1.000000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 0.51483\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 1.000000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 0.84864\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 1.000000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 0.64217\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 1.000000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 0.47010\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 1.000000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 1.24395\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 1.000000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 0.66669\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 1.000000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 0.47178\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.100000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 0.36401\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.100000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 0.37708\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.100000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 0.35434\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.100000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 0.39023\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.100000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 0.37288\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.100000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 0.34657\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.100000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 0.48435\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.100000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 0.36710\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.100000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 0.34356\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.100000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 0.36301\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.100000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 0.37960\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.100000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 0.34623\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.100000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 0.39399\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.100000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 0.36677\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.100000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 0.34133\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.100000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 0.50752\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.100000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 0.37281\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.100000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 0.34424\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.100000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 0.36316\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.100000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 0.37092\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.100000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 0.34658\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.100000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 0.40725\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.100000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 0.37358\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.100000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 0.34093\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.100000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 0.46806\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.100000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 0.41328\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.100000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 0.35397\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.010000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 0.50462\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.010000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 0.35701\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.010000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 0.33262\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.010000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 0.39581\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.010000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 0.34670\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.010000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 0.32934\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.010000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 0.41676\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.010000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 0.34589\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.010000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 0.33061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.010000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 0.50284\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.010000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 0.36157\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.010000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 0.33473\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.010000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 0.40915\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.010000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 0.34892\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.010000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 0.33178\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.010000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 0.42889\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.010000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 0.35402\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.010000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 0.33556\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.010000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 0.50934\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.010000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 0.36659\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.010000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 0.33682\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.010000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 0.40981\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.010000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 0.35600\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.010000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 0.33555\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.010000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 0.44791\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.010000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 0.36241\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.010000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 0.35090\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.001000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 1.89961\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.001000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 1.77525\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.001000, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 1.68269\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.001000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 1.81981\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.001000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 1.72476\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.001000, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 1.66408\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.001000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 1.79626\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.001000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 1.71422\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.001000, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 1.66129\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.001000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 1.89732\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.001000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 1.76453\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.001000, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 1.67659\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.001000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 1.81464\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.001000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 1.72087\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.001000, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 1.65783\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.001000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 1.79305\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.001000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 1.71003\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.001000, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 1.65633\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.001000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 1.89328\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.001000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 1.75946\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.001000, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 1.67169\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.001000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 1.81101\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.001000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 1.71611\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.001000, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 1.65437\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.001000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 1.79683\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.001000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 1.70776\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.001000, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 1.65542\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.000100, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 3.93577\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.000100, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 3.92584\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.000100, subsample = 0.500000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 3.91312\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.000100, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 3.92957\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.000100, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 3.92058\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.000100, subsample = 0.500000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 3.91013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.000100, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 3.92743\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.000100, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 3.91882\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.000100, subsample = 0.500000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 3.91013\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.000100, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 3.93530\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.000100, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 3.92501\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.000100, subsample = 0.750000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 3.91208\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.000100, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 3.92932\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.000100, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 3.91902\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.000100, subsample = 0.750000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 3.90919\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.000100, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 3.92702\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.000100, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 3.91826\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.000100, subsample = 0.750000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 3.90927\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.000100, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.400000, Score = 3.93542\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.000100, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 0.700000, Score = 3.92385\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.000100, subsample = 1.000000, colsample_bylevel = 0.400000, colsample_bytree = 1.000000, Score = 3.91149\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.000100, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.400000, Score = 3.92862\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.000100, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 0.700000, Score = 3.91903\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.000100, subsample = 1.000000, colsample_bylevel = 0.700000, colsample_bytree = 1.000000, Score = 3.90895\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.000100, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.400000, Score = 3.92731\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.000100, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 0.700000, Score = 3.91805\n",
      "n_estimators = 1000, max_depth = 100, learning_rate = 0.000100, subsample = 1.000000, colsample_bylevel = 1.000000, colsample_bytree = 1.000000, Score = 3.90912\n",
      "(1215, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>colsample_bylevel</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>score</th>\n",
       "      <th>subsample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>869</th>\n",
       "      <td>0.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>50</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.329324</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1139</th>\n",
       "      <td>0.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>100</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.329342</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>0.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>75</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.329342</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1007</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>75</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.330613</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1142</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>100</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.330613</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      colsample_bylevel  colsample_bytree  learning_rate  max_depth  \\\n",
       "869                 0.7               1.0           0.01         50   \n",
       "1139                0.7               1.0           0.01        100   \n",
       "1004                0.7               1.0           0.01         75   \n",
       "1007                1.0               1.0           0.01         75   \n",
       "1142                1.0               1.0           0.01        100   \n",
       "\n",
       "      n_estimators     score  subsample  \n",
       "869           1000  0.329324        0.5  \n",
       "1139          1000  0.329342        0.5  \n",
       "1004          1000  0.329342        0.5  \n",
       "1007          1000  0.330613        0.5  \n",
       "1142          1000  0.330613        0.5  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradient Boosting Machine 패키지인 XGBoost를 가져옵니다.\n",
    "# 이를 xgb라는 축약어로 사용합니다.\n",
    "import xgboost as xgb\n",
    "\n",
    "# scikit-learn 패키지의 model_selection 모듈에 있는 cross_val_score 함수를 가지고 옵니다.\n",
    "# 이 함수가 Cross Validation을 담당합니다.\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# n_estimators는 트리의 갯수입니다.\n",
    "# 너무 낮으면 트리의 갯수가 부족하고, 너무 높으면 트리가 과적합(Overfitting)되는 현상이 있습니다.\n",
    "# 그러므로 적당한 값을 주는 것이 좋습니다.\n",
    "n_estimators_list = [100, 300, 1000]\n",
    "\n",
    "# max_depth의 후보군을 지정합니다. 10 ~ 90 사이에서 10 단위로 지정하겠습니다.\n",
    "max_depth_list = [50, 75, 100]\n",
    "\n",
    "# learning_rate의 후보군을 지정합니다. 트리마다의 비중을 나타냅니다.\n",
    "# 보통은 10의 -n승 단위로 지정합니다.\n",
    "learning_rate_list = [1.0, 0.1, 0.01, 0.001, 0.0001]\n",
    "\n",
    "# subsample의 후보군을 지정합니다. 하나의 트리를 만들 때, 사용할 데이터의 비율을 나타냅니다.\n",
    "# 0.5, 0.75, 1.0을 주겠습니다. (각각 50%, 75%, 100%의 데이터를 사용합니다.)\n",
    "subsample_list = [0.5, 0.75, 1.0]\n",
    "\n",
    "# colsample_bytree의 후보군을 지정합니다. 하나의 트리를 만들 때, 사용할 컬럼의 비율을 나타냅니다.\n",
    "# 0.5, 0.75, 1.0을 주겠습니다. (각각 50%, 75%, 100%의 컬럼을 사용합니다.)\n",
    "colsample_bytree_list = [0.4, 0.7, 1.0]\n",
    "\n",
    "# colsample_bylevel의 후보군을 지정합니다. 나무에서 가지를 한 번 칠 때, 사용할 컬럼의 비율을 나타냅니다.\n",
    "# 0.5, 0.75, 1.0을 주겠습니다. (각각 50%, 75%, 100%의 컬럼을 사용합니다.)\n",
    "colsample_bylevel_list = [0.4, 0.7, 1.0]\n",
    "\n",
    "# hyperparameter 탐색 결과를 리스트로 저장합니다.\n",
    "hyperparameters_list = []\n",
    "\n",
    "# 모든 종류의 hyperparameter 후보군을 만듭니다.\n",
    "for n_estimators in n_estimators_list:\n",
    "    for max_depth in max_depth_list:\n",
    "        for learning_rate in learning_rate_list:\n",
    "            for subsample in subsample_list:\n",
    "                for colsample_bylevel in colsample_bylevel_list:\n",
    "                    for colsample_bytree in colsample_bytree_list:\n",
    "                        # XGBRegressor를 생성합니다. 실행할때는 다음의 옵션이 들어갑니다.\n",
    "                        # 1) n_estimators. 트리의 갯수입니다. 지정한 갯수만큼 트리를 생성합니다.\n",
    "                        # 2) max_depth. 트리의 깊이입니다. 지정한 숫자만큼 트리가 깊게 가지를 뻗습니다.\n",
    "                        # 3) learning_rate. 각 트리마다의 비중을 나타냅니다. 너무 작으면 과적합(overfitting)될 가능성이 있고, 너무 높으면 부적합(underfitting)될 가능성이 있습니다.\n",
    "                        # 4) subsample. 하나의 트리를 만들 때 사용할 데이터의 비율을 나타냅니다. 0.0 ~ 1.0 사이의 값을 넣으면 지정한 비율만큼만 랜덤하게 데이터를 사용합니다.\n",
    "                        # 5) colsample_bytree. 하나의 트리를 만들 때 사용할 feature의 비율을 나타냅니다. 0.0 ~ 1.0 사이의 값을 넣으면 트리를 만들 때 지정한 비율만큼만 랜덤하게 feature를 사용합니다.\n",
    "                        # 6) colsample_bylevel. 트리가 한 번 가지를 칠 때 사용할 feature의 비율을 나타냅니다. 0.0 ~ 1.0 사이의 값을 넣으면 트리가 가지를 칠 때 지정한 비율만큼만 랜덤하게 feature를 사용합니다.\n",
    "                        # 생성한 XGBRegressor를 model이라는 이름의 변수에 대입합니다.\n",
    "                        model = xgb.XGBRegressor(n_estimators=n_estimators,\n",
    "                                                 max_depth=max_depth,\n",
    "                                                 learning_rate=learning_rate,\n",
    "                                                 subsample=subsample,\n",
    "                                                 colsample_bytree=colsample_bytree,\n",
    "                                                 colsample_bylevel=colsample_bylevel,\n",
    "                                                 seed=37)\n",
    "\n",
    "                        # cross_val_score를 실행합니다. 실행할 때는 다음의 옵션이 들어갑니다.\n",
    "                        # 1) model. 점수를 측정할 머신러닝 모델이 들어갑니다.\n",
    "                        # 2) X_train. train 데이터의 feature 입니다.\n",
    "                        # 3) y_train. train 데이터의 label 입니다.\n",
    "                        # 4) cv. Cross Validation에서 데이터를 조각낼(split) 갯수입니다. 총 20조각을 내야하기 때문에 20을 대입합니다.\n",
    "                        # 5) scoring. 점수를 측정할 공식입니다. 앞서 구현한 RMSE를 적용합니다.\n",
    "                        # 마지막으로, 이 함수의 실행 결과의 평균(mean)을 구한 뒤 score라는 이름의 새로운 변수에 할당합니다.\n",
    "                        score = cross_val_score(model, X_train, y_train, cv=20, scoring=rmse_score).mean()\n",
    "\n",
    "                        # hyperparameter 탐색 결과를 딕셔너리화 합니다.\n",
    "                        hyperparameters = {\n",
    "                            'score': score,\n",
    "                            'n_estimators': n_estimators,\n",
    "                            'max_depth': max_depth,\n",
    "                            'learning_rate': learning_rate,\n",
    "                            'subsample': subsample,\n",
    "                            'colsample_bylevel': colsample_bylevel,\n",
    "                            'colsample_bytree': colsample_bytree,\n",
    "                        }\n",
    "\n",
    "                        # hyperparameter 탐색 결과를 리스트에 저장합니다.\n",
    "                        hyperparameters_list.append(hyperparameters)\n",
    "\n",
    "                        # hyperparameter 탐색 결과를 출력합니다.\n",
    "                        print(f\"n_estimators = {n_estimators}, max_depth = {max_depth:2}, learning_rate = {learning_rate:.6f}, subsample = {subsample:.6f}, colsample_bylevel = {colsample_bylevel:.6f}, colsample_bytree = {colsample_bytree:.6f}, Score = {score:.5f}\")\n",
    "\n",
    "\n",
    "# hyperparameters_list를 Pandas의 DataFrame으로 변환합니다.\n",
    "hyperparameters_list = pd.DataFrame.from_dict(hyperparameters_list)\n",
    "\n",
    "# 변환한 hyperparameters_list를 score가 낮은 순으로 정렬합니다.\n",
    "# (RMSE는 score가 낮을 수록 더 정확도가 높다고 가정합니다)\n",
    "hyperparameters_list = hyperparameters_list.sort_values(by=\"score\")\n",
    "\n",
    "# hyperparameters_list 변수에 할당된 데이터의 행렬 사이즈를 출력합니다.\n",
    "# 출력은 (row, column) 으로 표시됩니다.\n",
    "print(hyperparameters_list.shape)\n",
    "\n",
    "# hyperparameters_list의 상위 5개를 출력합니다.\n",
    "hyperparameters_list.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 2 - Coarse & Finer Search\n",
    "\n",
    "Grid Search는 굉장히 좋은 방식이지만 한 가지 큰 단점을 갖고 있습니다. 바로 **Grid Search로는 거의 대부분의 경우 가장 좋은 하이퍼패러미터를 찾을 수 없다는 사실**입니다.\n",
    "\n",
    "가령 [Bike Sharing Demand](https://www.kaggle.com/c/bike-sharing-demand) 문제에서 가장 적합한 max_depth가 83이라고 한다면, 위 Grid Search에서는 찾을 수 없습니다. 왜냐하면 위 코드에서 max_depth의 후보군을 다음과 같이 지정해놓았기 때문입니다.\n",
    "\n",
    "```\n",
    "# 이 후보군에는 max_depth가 83인 경우가 없습니다.\n",
    "max_depth_list = [50, 75, 100]\n",
    "```\n",
    "\n",
    "그러므로 Grid Search로는 가장 좋은 하이퍼패러미터에 근접한 다른 하이퍼패러미터는 찾을 수 있지만, 가장 좋은 하이퍼패러미터를 찾는 것은 어렵습니다.\n",
    "\n",
    "그렇다면 어떻게 하면 가장 좋은 하이퍼패러미터를 찾을 수 있을까요? 답은 간단합니다. 이론상으로 존재 가능한 모든 하이퍼패러미터 범위에서 랜덤하게 찾아서 Cross Validation을 해보면 됩니다. 이 방식을 랜덤 서치(Random Search)라고 합니다.\n",
    "\n",
    "<img src=\"http://cs231n.github.io/assets/nn3/gridsearchbad.jpeg\" alt=\"Random Search for Hyper-Parameter Optimization\" style=\"width: 360px;\"/>\n",
    "\n",
    "<p style=\"text-align: center;\">\n",
    "  <small>\n",
    "    위 그림과 같이, Grid Search를 활용하면 가장 좋은 성능을 내는 하이퍼패러미터를 찾기 어렵습니다. 이런 경우는 Random Search를 사용합니다.\n",
    "  </small>\n",
    "</p>\n",
    "<p style=\"text-align: center;\">\n",
    "  <small>\n",
    "    (see <a href=\"http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf\">Random Search for Hyper-Parameter Optimization</a>)\n",
    "  </small>\n",
    "</p>\n",
    "\n",
    "하지만 랜덤 서치(Random Search)는 현실적으로 시간이 오래 걸리기 때문에, 랜덤 서치(Random Search)를 응용한 다른 하이퍼패러미터 튜닝 방식을 사용하겠습니다. 바로 **Coarse & Finer Search** 입니다.\n",
    "\n",
    "Coarse & Finer Search는 크게 1) Coarse Search와 2) Finer Search로 동작합니다\n",
    "\n",
    "먼저 **Coarse Search**에서는 Random Search를 하되, 이론상으로 존재 가능한 모든 하이퍼패러미터 범위를 집어넣습니다. 이렇게 Random Search를 하면 가장 좋은 하이퍼패러미터를 찾는 것은 어렵지만, **좋지 않은 하이퍼패러미터를 정렬해서 후순위로 놓을 수 있습니다.**\n",
    "\n",
    "이를 통해 좋지 않은 하이퍼패러미터를 버린 뒤 다시 한 번 Random Search를 하는 것을 **Finer Search**라고 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Coarse Search **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 n_estimators = 699, max_depth = 24, learning_rate = 0.2322858010, subsample = 0.345621, colsample_bylevel = 0.768295, colsample_bytree = 0.477353, Score = 0.45069\n",
      " 1 n_estimators = 932, max_depth = 27, learning_rate = 0.0209459632, subsample = 0.289088, colsample_bylevel = 0.863977, colsample_bytree = 0.890338, Score = 0.32996\n",
      " 2 n_estimators = 138, max_depth = 68, learning_rate = 0.0000000047, subsample = 0.733244, colsample_bylevel = 0.943907, colsample_bytree = 0.656607, Score = 4.31058\n",
      " 3 n_estimators = 747, max_depth = 18, learning_rate = 0.0049878620, subsample = 0.715803, colsample_bylevel = 0.612042, colsample_bytree = 0.848196, Score = 0.37242\n",
      " 4 n_estimators = 149, max_depth = 49, learning_rate = 0.2041003999, subsample = 0.124723, colsample_bylevel = 0.886809, colsample_bytree = 0.988393, Score = 0.44243\n",
      " 5 n_estimators = 636, max_depth = 16, learning_rate = 0.0000000030, subsample = 0.504256, colsample_bylevel = 0.547654, colsample_bytree = 0.631089, Score = 4.31057\n",
      " 6 n_estimators = 181, max_depth = 13, learning_rate = 0.0791185412, subsample = 0.778489, colsample_bylevel = 0.897664, colsample_bytree = 0.930401, Score = 0.34003\n",
      " 7 n_estimators = 418, max_depth = 17, learning_rate = 0.1830706502, subsample = 0.310375, colsample_bylevel = 0.500772, colsample_bytree = 0.672945, Score = 0.39840\n",
      " 8 n_estimators = 503, max_depth = 21, learning_rate = 0.0000722441, subsample = 0.378579, colsample_bylevel = 0.739344, colsample_bytree = 0.938420, Score = 4.16130\n",
      " 9 n_estimators = 634, max_depth = 54, learning_rate = 0.0000013401, subsample = 0.403305, colsample_bylevel = 0.673181, colsample_bytree = 0.993229, Score = 4.30703\n",
      "10 n_estimators = 716, max_depth = 33, learning_rate = 0.0001939084, subsample = 0.781217, colsample_bylevel = 0.695672, colsample_bytree = 0.775315, Score = 3.77404\n",
      "11 n_estimators = 213, max_depth = 45, learning_rate = 0.0000000108, subsample = 0.572285, colsample_bylevel = 0.443255, colsample_bytree = 0.795412, Score = 4.31057\n",
      "12 n_estimators = 446, max_depth = 83, learning_rate = 0.0640465370, subsample = 0.150685, colsample_bylevel = 0.937346, colsample_bytree = 0.814871, Score = 0.35610\n",
      "13 n_estimators = 747, max_depth = 88, learning_rate = 0.0002217402, subsample = 0.726182, colsample_bylevel = 0.946189, colsample_bytree = 0.903247, Score = 3.67416\n",
      "14 n_estimators = 489, max_depth = 57, learning_rate = 0.0201989867, subsample = 0.475406, colsample_bylevel = 0.774680, colsample_bytree = 0.580041, Score = 0.36350\n",
      "15 n_estimators = 839, max_depth = 45, learning_rate = 0.5442394865, subsample = 0.995695, colsample_bylevel = 0.478000, colsample_bytree = 0.753065, Score = 0.42932\n",
      "16 n_estimators = 356, max_depth = 85, learning_rate = 0.0000000038, subsample = 0.373146, colsample_bylevel = 0.897079, colsample_bytree = 0.583530, Score = 4.31057\n",
      "17 n_estimators = 935, max_depth = 75, learning_rate = 0.9807720629, subsample = 0.893219, colsample_bylevel = 0.637073, colsample_bytree = 0.439554, Score = 0.74638\n",
      "18 n_estimators = 696, max_depth = 43, learning_rate = 0.0000000009, subsample = 0.653544, colsample_bylevel = 0.522975, colsample_bytree = 0.716282, Score = 4.31058\n",
      "19 n_estimators = 495, max_depth = 80, learning_rate = 0.0000030272, subsample = 0.973307, colsample_bylevel = 0.785242, colsample_bytree = 0.815694, Score = 4.30437\n",
      "20 n_estimators = 884, max_depth = 61, learning_rate = 0.0000023313, subsample = 0.399694, colsample_bylevel = 0.523210, colsample_bytree = 0.775448, Score = 4.30210\n",
      "21 n_estimators = 687, max_depth = 73, learning_rate = 0.0006810143, subsample = 0.829407, colsample_bylevel = 0.934895, colsample_bytree = 0.439556, Score = 2.81349\n",
      "22 n_estimators = 364, max_depth = 34, learning_rate = 0.0000103534, subsample = 0.172567, colsample_bylevel = 0.859149, colsample_bytree = 0.628469, Score = 4.29529\n",
      "23 n_estimators = 912, max_depth = 12, learning_rate = 0.0000019339, subsample = 0.621734, colsample_bylevel = 0.406301, colsample_bytree = 0.690087, Score = 4.30345\n",
      "24 n_estimators = 362, max_depth = 79, learning_rate = 0.0000005338, subsample = 0.354466, colsample_bylevel = 0.789681, colsample_bytree = 0.670166, Score = 4.30979\n",
      "25 n_estimators = 868, max_depth = 59, learning_rate = 0.0229278034, subsample = 0.404547, colsample_bylevel = 0.468473, colsample_bytree = 0.869623, Score = 0.33691\n",
      "26 n_estimators = 965, max_depth = 15, learning_rate = 0.0000004128, subsample = 0.939578, colsample_bylevel = 0.725404, colsample_bytree = 0.909890, Score = 4.30891\n",
      "27 n_estimators = 836, max_depth = 89, learning_rate = 0.0000037724, subsample = 0.726318, colsample_bylevel = 0.683683, colsample_bytree = 0.500918, Score = 4.29781\n",
      "28 n_estimators = 323, max_depth = 26, learning_rate = 0.0000000392, subsample = 0.978342, colsample_bylevel = 0.940089, colsample_bytree = 0.642947, Score = 4.31053\n",
      "29 n_estimators = 787, max_depth =  7, learning_rate = 0.0000995481, subsample = 0.381461, colsample_bylevel = 0.785578, colsample_bytree = 0.470377, Score = 4.00591\n",
      "30 n_estimators = 189, max_depth = 68, learning_rate = 0.0000167665, subsample = 0.877068, colsample_bylevel = 0.672164, colsample_bytree = 0.827566, Score = 4.29737\n",
      "31 n_estimators = 201, max_depth = 50, learning_rate = 0.0000001114, subsample = 0.433854, colsample_bylevel = 0.569141, colsample_bytree = 0.681251, Score = 4.31049\n",
      "32 n_estimators = 959, max_depth = 71, learning_rate = 0.0056328430, subsample = 0.714654, colsample_bylevel = 0.987466, colsample_bytree = 0.682794, Score = 0.36501\n",
      "33 n_estimators = 889, max_depth = 90, learning_rate = 0.0004388320, subsample = 0.382027, colsample_bylevel = 0.661549, colsample_bytree = 0.797469, Score = 2.97803\n",
      "34 n_estimators = 284, max_depth = 11, learning_rate = 0.0000000002, subsample = 0.602027, colsample_bylevel = 0.456392, colsample_bytree = 0.542921, Score = 4.31058\n",
      "35 n_estimators = 173, max_depth = 95, learning_rate = 0.0003143775, subsample = 0.368995, colsample_bylevel = 0.516508, colsample_bytree = 0.960402, Score = 4.09000\n",
      "36 n_estimators = 647, max_depth = 55, learning_rate = 0.0000000361, subsample = 0.828360, colsample_bylevel = 0.513000, colsample_bytree = 0.449799, Score = 4.31049\n",
      "37 n_estimators = 232, max_depth = 81, learning_rate = 0.0000003053, subsample = 0.595217, colsample_bylevel = 0.729356, colsample_bytree = 0.902786, Score = 4.31028\n",
      "38 n_estimators = 365, max_depth = 93, learning_rate = 0.0358787616, subsample = 0.898861, colsample_bylevel = 0.908478, colsample_bytree = 0.566676, Score = 0.38425\n",
      "39 n_estimators = 779, max_depth = 84, learning_rate = 0.0147565408, subsample = 0.306529, colsample_bylevel = 0.403381, colsample_bytree = 0.623426, Score = 0.35509\n",
      "40 n_estimators = 318, max_depth = 90, learning_rate = 0.0000000408, subsample = 0.241957, colsample_bylevel = 0.550880, colsample_bytree = 0.645874, Score = 4.31053\n",
      "41 n_estimators = 885, max_depth = 87, learning_rate = 0.0000048304, subsample = 0.150566, colsample_bylevel = 0.530487, colsample_bytree = 0.646527, Score = 4.29327\n",
      "42 n_estimators = 774, max_depth =  4, learning_rate = 0.3656108884, subsample = 0.999501, colsample_bylevel = 0.943843, colsample_bytree = 0.491711, Score = 0.34537\n",
      "43 n_estimators = 682, max_depth = 89, learning_rate = 0.0000037424, subsample = 0.590769, colsample_bylevel = 0.545058, colsample_bytree = 0.546107, Score = 4.30020\n",
      "44 n_estimators = 629, max_depth = 29, learning_rate = 0.0000204281, subsample = 0.930338, colsample_bylevel = 0.578545, colsample_bytree = 0.630497, Score = 4.25849\n",
      "45 n_estimators = 829, max_depth = 60, learning_rate = 0.0009029055, subsample = 0.608412, colsample_bylevel = 0.524489, colsample_bytree = 0.696356, Score = 2.15942\n",
      "46 n_estimators = 810, max_depth = 89, learning_rate = 0.0000000492, subsample = 0.272125, colsample_bylevel = 0.980866, colsample_bytree = 0.917275, Score = 4.31041\n",
      "47 n_estimators = 403, max_depth =  5, learning_rate = 0.0399934453, subsample = 0.651052, colsample_bylevel = 0.467062, colsample_bytree = 0.835914, Score = 0.32159\n",
      "48 n_estimators = 857, max_depth = 10, learning_rate = 0.0000002220, subsample = 0.868509, colsample_bylevel = 0.453792, colsample_bytree = 0.434522, Score = 4.30983\n",
      "49 n_estimators = 703, max_depth = 45, learning_rate = 0.0044924140, subsample = 0.721587, colsample_bylevel = 0.635406, colsample_bytree = 0.867280, Score = 0.42002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 n_estimators = 426, max_depth = 27, learning_rate = 0.1464556134, subsample = 0.543915, colsample_bylevel = 0.473488, colsample_bytree = 0.733188, Score = 0.37602\n",
      "51 n_estimators = 699, max_depth =  7, learning_rate = 0.0262003173, subsample = 0.513865, colsample_bylevel = 0.631298, colsample_bytree = 0.971718, Score = 0.32117\n",
      "52 n_estimators = 400, max_depth = 76, learning_rate = 0.0000000044, subsample = 0.244632, colsample_bylevel = 0.507006, colsample_bytree = 0.634038, Score = 4.31057\n",
      "53 n_estimators = 192, max_depth = 89, learning_rate = 0.0000179771, subsample = 0.916516, colsample_bylevel = 0.892634, colsample_bytree = 0.900298, Score = 4.29620\n",
      "54 n_estimators = 814, max_depth =  3, learning_rate = 0.1425177568, subsample = 0.250722, colsample_bylevel = 0.681453, colsample_bytree = 0.840537, Score = 0.33576\n",
      "55 n_estimators = 501, max_depth = 21, learning_rate = 0.0000000450, subsample = 0.145558, colsample_bylevel = 0.755875, colsample_bytree = 0.601645, Score = 4.31049\n",
      "56 n_estimators = 331, max_depth = 93, learning_rate = 0.1181079429, subsample = 0.334909, colsample_bylevel = 0.854929, colsample_bytree = 0.558321, Score = 0.40052\n",
      "57 n_estimators = 123, max_depth = 29, learning_rate = 0.0002807910, subsample = 0.561204, colsample_bylevel = 0.599664, colsample_bytree = 0.627916, Score = 4.17255\n",
      "58 n_estimators = 582, max_depth = 55, learning_rate = 0.0000000001, subsample = 0.992588, colsample_bylevel = 0.960817, colsample_bytree = 0.538212, Score = 4.31058\n",
      "59 n_estimators = 377, max_depth = 55, learning_rate = 0.7438589701, subsample = 0.356307, colsample_bylevel = 0.685136, colsample_bytree = 0.598843, Score = 1.10299\n",
      "60 n_estimators = 918, max_depth =  9, learning_rate = 0.0006800793, subsample = 0.378812, colsample_bylevel = 0.869626, colsample_bytree = 0.854009, Score = 2.37986\n",
      "61 n_estimators = 490, max_depth = 29, learning_rate = 0.0000047022, subsample = 0.810455, colsample_bylevel = 0.510392, colsample_bytree = 0.761150, Score = 4.30107\n",
      "62 n_estimators = 710, max_depth = 19, learning_rate = 0.0027408782, subsample = 0.991736, colsample_bylevel = 0.564205, colsample_bytree = 0.718276, Score = 0.83501\n",
      "63 n_estimators = 761, max_depth = 85, learning_rate = 0.0192320397, subsample = 0.748591, colsample_bylevel = 0.691748, colsample_bytree = 0.589797, Score = 0.36735\n",
      "64 n_estimators = 728, max_depth = 60, learning_rate = 0.1412635723, subsample = 0.695288, colsample_bylevel = 0.710799, colsample_bytree = 0.593679, Score = 0.42517\n",
      "65 n_estimators = 316, max_depth = 27, learning_rate = 0.1471100928, subsample = 0.145800, colsample_bylevel = 0.648763, colsample_bytree = 0.605091, Score = 0.42332\n",
      "66 n_estimators = 587, max_depth = 67, learning_rate = 0.0000000074, subsample = 0.420016, colsample_bylevel = 0.650798, colsample_bytree = 0.498415, Score = 4.31056\n",
      "67 n_estimators = 945, max_depth = 19, learning_rate = 0.1531371146, subsample = 0.548602, colsample_bylevel = 0.731302, colsample_bytree = 0.496670, Score = 0.43069\n",
      "68 n_estimators = 461, max_depth = 58, learning_rate = 0.0000000199, subsample = 0.857297, colsample_bylevel = 0.849168, colsample_bytree = 0.674343, Score = 4.31054\n",
      "69 n_estimators = 506, max_depth = 57, learning_rate = 0.0000000130, subsample = 0.330269, colsample_bylevel = 0.442864, colsample_bytree = 0.888845, Score = 4.31055\n",
      "70 n_estimators = 519, max_depth = 15, learning_rate = 0.0000000345, subsample = 0.984410, colsample_bylevel = 0.412342, colsample_bytree = 0.678033, Score = 4.31051\n",
      "71 n_estimators = 562, max_depth = 88, learning_rate = 0.0002139609, subsample = 0.208272, colsample_bylevel = 0.731471, colsample_bytree = 0.878480, Score = 3.84127\n",
      "72 n_estimators = 134, max_depth = 30, learning_rate = 0.0003101267, subsample = 0.552828, colsample_bylevel = 0.732190, colsample_bytree = 0.838838, Score = 4.14081\n",
      "73 n_estimators = 633, max_depth = 58, learning_rate = 0.0000264021, subsample = 0.443676, colsample_bylevel = 0.990289, colsample_bytree = 0.972268, Score = 4.24121\n",
      "74 n_estimators = 473, max_depth = 63, learning_rate = 0.0000002064, subsample = 0.448687, colsample_bylevel = 0.608312, colsample_bytree = 0.462555, Score = 4.31018\n",
      "75 n_estimators = 380, max_depth = 84, learning_rate = 0.0000199118, subsample = 0.454644, colsample_bylevel = 0.499235, colsample_bytree = 0.507499, Score = 4.28029\n",
      "76 n_estimators = 132, max_depth = 75, learning_rate = 0.0043060072, subsample = 0.208642, colsample_bylevel = 0.651713, colsample_bytree = 0.876266, Score = 2.51666\n",
      "77 n_estimators = 429, max_depth =  6, learning_rate = 0.0069815001, subsample = 0.964905, colsample_bylevel = 0.952162, colsample_bytree = 0.947500, Score = 0.44311\n",
      "78 n_estimators = 347, max_depth = 47, learning_rate = 0.0093663548, subsample = 0.139800, colsample_bylevel = 0.932781, colsample_bytree = 0.838805, Score = 0.41824\n",
      "79 n_estimators = 766, max_depth = 93, learning_rate = 0.0000000118, subsample = 0.903499, colsample_bylevel = 0.840421, colsample_bytree = 0.792772, Score = 4.31054\n",
      "80 n_estimators = 742, max_depth = 71, learning_rate = 0.0888087065, subsample = 0.884192, colsample_bylevel = 0.527225, colsample_bytree = 0.982032, Score = 0.34298\n",
      "81 n_estimators = 338, max_depth = 82, learning_rate = 0.0000000320, subsample = 0.282353, colsample_bylevel = 0.992051, colsample_bytree = 0.792277, Score = 4.31053\n",
      "82 n_estimators = 433, max_depth = 73, learning_rate = 0.0000001165, subsample = 0.248062, colsample_bylevel = 0.754374, colsample_bytree = 0.428163, Score = 4.31038\n",
      "83 n_estimators = 743, max_depth = 50, learning_rate = 0.0302944539, subsample = 0.321904, colsample_bylevel = 0.551960, colsample_bytree = 0.774964, Score = 0.33755\n",
      "84 n_estimators = 976, max_depth = 74, learning_rate = 0.0000000002, subsample = 0.441274, colsample_bylevel = 0.543013, colsample_bytree = 0.557282, Score = 4.31058\n",
      "85 n_estimators = 816, max_depth = 24, learning_rate = 0.1196689432, subsample = 0.576076, colsample_bylevel = 0.960542, colsample_bytree = 0.880688, Score = 0.35638\n",
      "86 n_estimators = 358, max_depth = 29, learning_rate = 0.0000000147, subsample = 0.773707, colsample_bylevel = 0.554307, colsample_bytree = 0.758348, Score = 4.31056\n",
      "87 n_estimators = 139, max_depth = 33, learning_rate = 0.0000108395, subsample = 0.261408, colsample_bylevel = 0.865924, colsample_bytree = 0.579585, Score = 4.30444\n",
      "88 n_estimators = 813, max_depth = 19, learning_rate = 0.0000137022, subsample = 0.936285, colsample_bylevel = 0.743037, colsample_bytree = 0.481291, Score = 4.26563\n",
      "89 n_estimators = 359, max_depth = 34, learning_rate = 0.5547208781, subsample = 0.880080, colsample_bylevel = 0.827680, colsample_bytree = 0.937673, Score = 0.45891\n",
      "90 n_estimators = 479, max_depth = 99, learning_rate = 0.0000000174, subsample = 0.516692, colsample_bylevel = 0.729623, colsample_bytree = 0.909218, Score = 4.31054\n",
      "91 n_estimators = 548, max_depth =  9, learning_rate = 0.0000000037, subsample = 0.658963, colsample_bylevel = 0.899043, colsample_bytree = 0.989653, Score = 4.31057\n",
      "92 n_estimators = 100, max_depth = 81, learning_rate = 0.0038433268, subsample = 0.173474, colsample_bylevel = 0.762391, colsample_bytree = 0.952989, Score = 2.98189\n",
      "93 n_estimators = 870, max_depth = 20, learning_rate = 0.0000178511, subsample = 0.589696, colsample_bylevel = 0.615853, colsample_bytree = 0.515258, Score = 4.24814\n",
      "94 n_estimators = 319, max_depth =  6, learning_rate = 0.0000000352, subsample = 0.931856, colsample_bylevel = 0.640521, colsample_bytree = 0.729642, Score = 4.31053\n",
      "95 n_estimators = 371, max_depth = 65, learning_rate = 0.3309241398, subsample = 0.259872, colsample_bylevel = 0.928881, colsample_bytree = 0.499068, Score = 0.60782\n",
      "96 n_estimators = 189, max_depth = 15, learning_rate = 0.1458215699, subsample = 0.560602, colsample_bylevel = 0.998627, colsample_bytree = 0.884252, Score = 0.36210\n",
      "97 n_estimators = 236, max_depth = 84, learning_rate = 0.0000000095, subsample = 0.516492, colsample_bylevel = 0.429003, colsample_bytree = 0.505152, Score = 4.31057\n",
      "98 n_estimators = 965, max_depth = 21, learning_rate = 0.0000005806, subsample = 0.675574, colsample_bylevel = 0.414357, colsample_bytree = 0.790556, Score = 4.30827\n",
      "99 n_estimators = 810, max_depth = 84, learning_rate = 0.0000032186, subsample = 0.138561, colsample_bylevel = 0.617605, colsample_bytree = 0.724864, Score = 4.29998\n",
      "(100, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>colsample_bylevel</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>epoch</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>score</th>\n",
       "      <th>subsample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.631298</td>\n",
       "      <td>0.971718</td>\n",
       "      <td>51</td>\n",
       "      <td>0.026200</td>\n",
       "      <td>7</td>\n",
       "      <td>699</td>\n",
       "      <td>0.321169</td>\n",
       "      <td>0.513865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.467062</td>\n",
       "      <td>0.835914</td>\n",
       "      <td>47</td>\n",
       "      <td>0.039993</td>\n",
       "      <td>5</td>\n",
       "      <td>403</td>\n",
       "      <td>0.321592</td>\n",
       "      <td>0.651052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.863977</td>\n",
       "      <td>0.890338</td>\n",
       "      <td>1</td>\n",
       "      <td>0.020946</td>\n",
       "      <td>27</td>\n",
       "      <td>932</td>\n",
       "      <td>0.329961</td>\n",
       "      <td>0.289088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.681453</td>\n",
       "      <td>0.840537</td>\n",
       "      <td>54</td>\n",
       "      <td>0.142518</td>\n",
       "      <td>3</td>\n",
       "      <td>814</td>\n",
       "      <td>0.335763</td>\n",
       "      <td>0.250722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.468473</td>\n",
       "      <td>0.869623</td>\n",
       "      <td>25</td>\n",
       "      <td>0.022928</td>\n",
       "      <td>59</td>\n",
       "      <td>868</td>\n",
       "      <td>0.336911</td>\n",
       "      <td>0.404547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.551960</td>\n",
       "      <td>0.774964</td>\n",
       "      <td>83</td>\n",
       "      <td>0.030294</td>\n",
       "      <td>50</td>\n",
       "      <td>743</td>\n",
       "      <td>0.337546</td>\n",
       "      <td>0.321904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.897664</td>\n",
       "      <td>0.930401</td>\n",
       "      <td>6</td>\n",
       "      <td>0.079119</td>\n",
       "      <td>13</td>\n",
       "      <td>181</td>\n",
       "      <td>0.340025</td>\n",
       "      <td>0.778489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.527225</td>\n",
       "      <td>0.982032</td>\n",
       "      <td>80</td>\n",
       "      <td>0.088809</td>\n",
       "      <td>71</td>\n",
       "      <td>742</td>\n",
       "      <td>0.342985</td>\n",
       "      <td>0.884192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.943843</td>\n",
       "      <td>0.491711</td>\n",
       "      <td>42</td>\n",
       "      <td>0.365611</td>\n",
       "      <td>4</td>\n",
       "      <td>774</td>\n",
       "      <td>0.345367</td>\n",
       "      <td>0.999501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.403381</td>\n",
       "      <td>0.623426</td>\n",
       "      <td>39</td>\n",
       "      <td>0.014757</td>\n",
       "      <td>84</td>\n",
       "      <td>779</td>\n",
       "      <td>0.355085</td>\n",
       "      <td>0.306529</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    colsample_bylevel  colsample_bytree  epoch  learning_rate  max_depth  \\\n",
       "51           0.631298          0.971718     51       0.026200          7   \n",
       "47           0.467062          0.835914     47       0.039993          5   \n",
       "1            0.863977          0.890338      1       0.020946         27   \n",
       "54           0.681453          0.840537     54       0.142518          3   \n",
       "25           0.468473          0.869623     25       0.022928         59   \n",
       "83           0.551960          0.774964     83       0.030294         50   \n",
       "6            0.897664          0.930401      6       0.079119         13   \n",
       "80           0.527225          0.982032     80       0.088809         71   \n",
       "42           0.943843          0.491711     42       0.365611          4   \n",
       "39           0.403381          0.623426     39       0.014757         84   \n",
       "\n",
       "    n_estimators     score  subsample  \n",
       "51           699  0.321169   0.513865  \n",
       "47           403  0.321592   0.651052  \n",
       "1            932  0.329961   0.289088  \n",
       "54           814  0.335763   0.250722  \n",
       "25           868  0.336911   0.404547  \n",
       "83           743  0.337546   0.321904  \n",
       "6            181  0.340025   0.778489  \n",
       "80           742  0.342985   0.884192  \n",
       "42           774  0.345367   0.999501  \n",
       "39           779  0.355085   0.306529  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradient Boosting Machine 패키지인 XGBoost를 가져옵니다.\n",
    "# 이를 xgb라는 축약어로 사용합니다.\n",
    "import xgboost as xgb\n",
    "\n",
    "# scikit-learn 패키지의 model_selection 모듈에 있는 cross_val_score 함수를 가지고 옵니다.\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# 랜덤 서치를 반복할 횟수입니다.\n",
    "# 보통 100번을 반복합니다.\n",
    "num_epoch = 100\n",
    "\n",
    "# hyperparameter 탐색 결과를 리스트로 저장합니다.\n",
    "coarse_hyperparameters_list = []\n",
    "\n",
    "# num_epoch 횟수만큼 랜덤 서치를 반복합니다.\n",
    "for epoch in range(num_epoch):\n",
    "    # 10에서 100 사이의 정수형(int) 값을 랜덤하게 생성하여 n_estimators 변수에 할당합니다.\n",
    "    n_estimators = np.random.randint(low=100, high=1000)\n",
    "\n",
    "    # 2에서 100 사이의 정수형(int) 값을 랜덤하게 생성하여 max_depth 변수에 할당합니다.\n",
    "    max_depth = np.random.randint(low=2, high=100)\n",
    "\n",
    "    # 1.0에서 1-e10(10의 -10승)사이의 실수형(float) 값을 랜덤하게 생성하여  learning_rate 변수에 할당합니다.\n",
    "    learning_rate = 10 ** -np.random.uniform(low=0, high=10)\n",
    "\n",
    "    # 0.1에서 1.0사이의 실수형(float) 값을 랜덤하게 생성하여 subsample 변수에 할당합니다.\n",
    "    subsample = np.random.uniform(low=0.1, high=1.0)\n",
    "\n",
    "    # 0.4에서 1.0사이의 실수형(float) 값을 랜덤하게 생성하여 colsample_bytree 변수에 할당합니다.\n",
    "    colsample_bytree = np.random.uniform(low=0.4, high=1.0)\n",
    "\n",
    "    # 0.4에서 1.0사이의 실수형(float) 값을 랜덤하게 생성하여 colsample_bylevel 변수에 할당합니다.\n",
    "    colsample_bylevel = np.random.uniform(low=0.4, high=1.0)\n",
    "\n",
    "    # XGBRegressor를 생성합니다. 실행할때는 다음의 옵션이 들어갑니다.\n",
    "    # 1) n_estimators. 트리의 갯수입니다. 지정한 갯수만큼 트리를 생성합니다.\n",
    "    # 2) max_depth. 트리의 깊이입니다. 지정한 숫자만큼 트리가 깊게 가지를 뻗습니다.\n",
    "    # 3) learning_rate. 각 트리마다의 비중을 나타냅니다. 너무 작으면 과적합(overfitting)될 가능성이 있고, 너무 높으면 부적합(underfitting)될 가능성이 있습니다.\n",
    "    # 4) subsample. 하나의 트리를 만들 때 사용할 데이터의 비율을 나타냅니다. 0.0 ~ 1.0 사이의 값을 넣으면 지정한 비율만큼만 랜덤하게 데이터를 사용합니다.\n",
    "    # 5) colsample_bytree. 하나의 트리를 만들 때 사용할 feature의 비율을 나타냅니다. 0.0 ~ 1.0 사이의 값을 넣으면 트리를 만들 때 지정한 비율만큼만 랜덤하게 feature를 사용합니다.\n",
    "    # 6) colsample_bylevel. 트리가 한 번 가지를 칠 때 사용할 feature의 비율을 나타냅니다. 0.0 ~ 1.0 사이의 값을 넣으면 트리가 가지를 칠 때 지정한 비율만큼만 랜덤하게 feature를 사용합니다.\n",
    "    # 생성한 XGBRegressor를 model이라는 이름의 변수에 대입합니다.\n",
    "    model = xgb.XGBRegressor(n_estimators=n_estimators,\n",
    "                             max_depth=max_depth,\n",
    "                             learning_rate=learning_rate,\n",
    "                             subsample=subsample,\n",
    "                             colsample_bylevel=colsample_bylevel,\n",
    "                             colsample_bytree=colsample_bytree,\n",
    "                             seed=37)\n",
    "\n",
    "    # cross_val_score를 실행합니다. 실행할 때는 다음의 옵션이 들어갑니다.\n",
    "    # 1) model. 점수를 측정할 머신러닝 모델이 들어갑니다.\n",
    "    # 2) X_train. train 데이터의 feature 입니다.\n",
    "    # 3) y_train. train 데이터의 label 입니다.\n",
    "    # 4) cv. Cross Validation에서 데이터를 조각낼(split) 갯수입니다. 총 20조각을 내야하기 때문에 20을 대입합니다.\n",
    "    # 5) scoring. 점수를 측정할 공식입니다. 앞서 구현한 RMSE를 적용합니다.\n",
    "    # 마지막으로, 이 함수의 실행 결과의 평균(mean)을 구한 뒤 score라는 이름의 새로운 변수에 할당합니다.\n",
    "    score = cross_val_score(model, X_train, y_train, cv=20, scoring=rmse_score).mean()\n",
    "    \n",
    "    # hyperparameter 탐색 결과를 딕셔너리화 합니다.\n",
    "    hyperparameters = {\n",
    "        'epoch': epoch,\n",
    "        'score': score,\n",
    "        'n_estimators': n_estimators,\n",
    "        'max_depth': max_depth,\n",
    "        'learning_rate': learning_rate,\n",
    "        'subsample': subsample,\n",
    "        'colsample_bylevel': colsample_bylevel,\n",
    "        'colsample_bytree': colsample_bytree,\n",
    "    }\n",
    "\n",
    "    # hyperparameter 탐색 결과를 리스트에 저장합니다.\n",
    "    coarse_hyperparameters_list.append(hyperparameters)\n",
    "\n",
    "    # hyperparameter 탐색 결과를 출력합니다.\n",
    "    print(f\"{epoch:2} n_estimators = {n_estimators}, max_depth = {max_depth:2}, learning_rate = {learning_rate:.10f}, subsample = {subsample:.6f}, colsample_bylevel = {colsample_bylevel:.6f}, colsample_bytree = {colsample_bytree:.6f}, Score = {score:.5f}\")\n",
    "\n",
    "\n",
    "# coarse_hyperparameters_list를 Pandas의 DataFrame으로 변환합니다.\n",
    "coarse_hyperparameters_list = pd.DataFrame.from_dict(coarse_hyperparameters_list)\n",
    "\n",
    "# 변환한 coarse_hyperparameters_list를 score가 낮은 순으로 정렬합니다.\n",
    "# (RMSE는 score가 낮을 수록 더 정확도가 높다고 가정합니다)\n",
    "coarse_hyperparameters_list = coarse_hyperparameters_list.sort_values(by=\"score\")\n",
    "\n",
    "# coarse_hyperparameters_list 변수에 할당된 데이터의 행렬 사이즈를 출력합니다.\n",
    "# 출력은 (row, column) 으로 표시됩니다.\n",
    "print(coarse_hyperparameters_list.shape)\n",
    "\n",
    "# coarse_hyperparameters_list의 상위 10개를 출력합니다.\n",
    "coarse_hyperparameters_list.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coarse Search가 끝났으면, 상위 5 ~ 10개의 결과만 출력한 뒤 이 결과를 낸 하이퍼패러미터 범위만 남겨놓고 다시 한 번 Random Search를 합니다. 이를 Finer Search라고 합니다.\n",
    "\n",
    "가령 위 Coarse Search를 통해, 다음의 하이퍼패러미터가 상위 5 ~ 10개 안에 들었다고 가정하겠습니다.\n",
    "\n",
    "  * n_estimators = 300 ~ 1,000\n",
    "  * max_depth = 2 ~ 60\n",
    "  * learning_rate = 1.0 ~ 0.01\n",
    "  * subsample = 0.2 ~ 0.7\n",
    "  * colsample_bytree = 0.7 ~ 1.0\n",
    "  * colsample_bylevel = 0.4 ~ 1.0\n",
    "  \n",
    "이제 위 코드를 그대로 사용하되, 다음의 부분만 수정한 뒤 다시 한 번 Random Search를 하겠습니다.\n",
    "\n",
    "```\n",
    "# 300에서 1000 사이의 정수형(int) 값을 랜덤하게 생성하여 n_estimators 변수에 할당합니다.\n",
    "n_estimators = np.random.randint(low=300, high=1000)\n",
    "\n",
    "# 2에서 60 사이의 정수형(int) 값을 랜덤하게 생성하여 max_depth 변수에 할당합니다.\n",
    "max_depth = np.random.randint(low=2, high=60)\n",
    "\n",
    "# 1.0에서 1-e2(10의 -2승)사이의 실수형(float) 값을 랜덤하게 생성하여  learning_rate 변수에 할당합니다.\n",
    "learning_rate = 10 ** -np.random.uniform(low=0, high=2)\n",
    "\n",
    "# 0.2에서 0.7사이의 실수형(float) 값을 랜덤하게 생성하여 subsample 변수에 할당합니다.\n",
    "subsample = np.random.uniform(low=0.2, high=0.7)\n",
    "\n",
    "# 0.7에서 1.0사이의 실수형(float) 값을 랜덤하게 생성하여 colsample_bytree 변수에 할당합니다.\n",
    "colsample_bytree = np.random.uniform(low=0.7, high=1.0)\n",
    "\n",
    "# 0.4에서 1.0사이의 실수형(float) 값을 랜덤하게 생성하여 colsample_bylevel 변수에 할당합니다.\n",
    "colsample_bylevel = np.random.uniform(low=0.4, high=1.0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Finer Search **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 n_estimators = 826, max_depth = 26, learning_rate = 0.0140914189, subsample = 0.604440, colsample_bylevel = 0.803958, colsample_bytree = 0.958276, Score = 0.33072\n",
      " 1 n_estimators = 693, max_depth = 19, learning_rate = 0.0204163085, subsample = 0.281243, colsample_bylevel = 0.842756, colsample_bytree = 0.922510, Score = 0.32868\n",
      " 2 n_estimators = 578, max_depth = 21, learning_rate = 0.0367275403, subsample = 0.673859, colsample_bylevel = 0.694686, colsample_bytree = 0.775183, Score = 0.34194\n",
      " 3 n_estimators = 556, max_depth = 35, learning_rate = 0.0206988792, subsample = 0.311399, colsample_bylevel = 0.730884, colsample_bytree = 0.920663, Score = 0.32868\n",
      " 4 n_estimators = 786, max_depth = 11, learning_rate = 0.0352740882, subsample = 0.394929, colsample_bylevel = 0.497259, colsample_bytree = 0.946960, Score = 0.33906\n",
      " 5 n_estimators = 462, max_depth =  8, learning_rate = 0.0217484971, subsample = 0.601801, colsample_bylevel = 0.435530, colsample_bytree = 0.907742, Score = 0.32755\n",
      " 6 n_estimators = 315, max_depth = 57, learning_rate = 0.0194657250, subsample = 0.313923, colsample_bylevel = 0.845876, colsample_bytree = 0.966426, Score = 0.32730\n",
      " 7 n_estimators = 943, max_depth = 21, learning_rate = 0.0697425019, subsample = 0.388384, colsample_bylevel = 0.547449, colsample_bytree = 0.899230, Score = 0.34735\n",
      " 8 n_estimators = 791, max_depth = 35, learning_rate = 0.0125012079, subsample = 0.224484, colsample_bylevel = 0.677926, colsample_bytree = 0.709716, Score = 0.33583\n",
      " 9 n_estimators = 511, max_depth = 35, learning_rate = 0.0178360559, subsample = 0.387557, colsample_bylevel = 0.623901, colsample_bytree = 0.816696, Score = 0.33536\n",
      "10 n_estimators = 898, max_depth = 21, learning_rate = 0.0324601521, subsample = 0.264827, colsample_bylevel = 0.875968, colsample_bytree = 0.713651, Score = 0.34599\n",
      "11 n_estimators = 892, max_depth = 20, learning_rate = 0.0910118678, subsample = 0.343863, colsample_bylevel = 0.896878, colsample_bytree = 0.891140, Score = 0.34970\n",
      "12 n_estimators = 406, max_depth = 14, learning_rate = 0.0121356539, subsample = 0.526529, colsample_bylevel = 0.546882, colsample_bytree = 0.752584, Score = 0.34846\n",
      "13 n_estimators = 683, max_depth = 22, learning_rate = 0.6734879598, subsample = 0.278793, colsample_bylevel = 0.832358, colsample_bytree = 0.743327, Score = 0.82619\n",
      "14 n_estimators = 565, max_depth = 56, learning_rate = 0.4208953969, subsample = 0.501257, colsample_bylevel = 0.686633, colsample_bytree = 0.919102, Score = 0.43834\n",
      "15 n_estimators = 895, max_depth = 55, learning_rate = 0.0229405212, subsample = 0.245841, colsample_bylevel = 0.720467, colsample_bytree = 0.926856, Score = 0.33425\n",
      "16 n_estimators = 441, max_depth =  5, learning_rate = 0.1557808729, subsample = 0.201518, colsample_bylevel = 0.604204, colsample_bytree = 0.789971, Score = 0.35362\n",
      "17 n_estimators = 838, max_depth = 40, learning_rate = 0.3109060394, subsample = 0.405133, colsample_bylevel = 0.874428, colsample_bytree = 0.816343, Score = 0.39935\n",
      "18 n_estimators = 426, max_depth = 56, learning_rate = 0.0980670795, subsample = 0.535718, colsample_bylevel = 0.775129, colsample_bytree = 0.742462, Score = 0.35147\n",
      "19 n_estimators = 372, max_depth = 30, learning_rate = 0.1709669643, subsample = 0.592810, colsample_bylevel = 0.496650, colsample_bytree = 0.802357, Score = 0.36893\n",
      "20 n_estimators = 636, max_depth =  9, learning_rate = 0.9526417597, subsample = 0.398907, colsample_bylevel = 0.844800, colsample_bytree = 0.733590, Score = 1.43410\n",
      "21 n_estimators = 481, max_depth = 49, learning_rate = 0.5854043195, subsample = 0.497239, colsample_bylevel = 0.782827, colsample_bytree = 0.719362, Score = 0.56249\n",
      "22 n_estimators = 864, max_depth = 22, learning_rate = 0.2735823453, subsample = 0.335316, colsample_bylevel = 0.762807, colsample_bytree = 0.792830, Score = 0.40885\n",
      "23 n_estimators = 916, max_depth = 51, learning_rate = 0.0100133400, subsample = 0.459323, colsample_bylevel = 0.695348, colsample_bytree = 0.747032, Score = 0.33249\n",
      "24 n_estimators = 640, max_depth = 23, learning_rate = 0.0499691162, subsample = 0.685181, colsample_bylevel = 0.472997, colsample_bytree = 0.925317, Score = 0.33838\n",
      "25 n_estimators = 738, max_depth = 45, learning_rate = 0.0407032864, subsample = 0.296325, colsample_bylevel = 0.894751, colsample_bytree = 0.963906, Score = 0.33906\n",
      "26 n_estimators = 933, max_depth = 29, learning_rate = 0.1886915092, subsample = 0.477641, colsample_bylevel = 0.693674, colsample_bytree = 0.958425, Score = 0.36226\n",
      "27 n_estimators = 385, max_depth = 54, learning_rate = 0.0254351149, subsample = 0.266720, colsample_bylevel = 0.831264, colsample_bytree = 0.867055, Score = 0.32644\n",
      "28 n_estimators = 701, max_depth = 18, learning_rate = 0.0146441339, subsample = 0.284567, colsample_bylevel = 0.791547, colsample_bytree = 0.716776, Score = 0.33701\n",
      "29 n_estimators = 388, max_depth = 43, learning_rate = 0.0232468029, subsample = 0.526676, colsample_bylevel = 0.558939, colsample_bytree = 0.901665, Score = 0.33353\n",
      "30 n_estimators = 715, max_depth = 57, learning_rate = 0.2204953869, subsample = 0.455188, colsample_bylevel = 0.712494, colsample_bytree = 0.994391, Score = 0.37042\n",
      "31 n_estimators = 982, max_depth = 21, learning_rate = 0.0394635889, subsample = 0.567685, colsample_bylevel = 0.850578, colsample_bytree = 0.998446, Score = 0.33515\n",
      "32 n_estimators = 554, max_depth = 34, learning_rate = 0.0167896270, subsample = 0.425722, colsample_bylevel = 0.684372, colsample_bytree = 0.971310, Score = 0.32618\n",
      "33 n_estimators = 639, max_depth = 56, learning_rate = 0.0342627969, subsample = 0.537765, colsample_bylevel = 0.614150, colsample_bytree = 0.786516, Score = 0.34602\n",
      "34 n_estimators = 345, max_depth = 42, learning_rate = 0.0156133893, subsample = 0.688611, colsample_bylevel = 0.469881, colsample_bytree = 0.799938, Score = 0.36028\n",
      "35 n_estimators = 855, max_depth =  8, learning_rate = 0.0285197555, subsample = 0.402216, colsample_bylevel = 0.547196, colsample_bytree = 0.854189, Score = 0.32445\n",
      "36 n_estimators = 538, max_depth = 44, learning_rate = 0.1027719906, subsample = 0.343363, colsample_bylevel = 0.456526, colsample_bytree = 0.729607, Score = 0.36177\n",
      "37 n_estimators = 866, max_depth = 14, learning_rate = 0.4914480158, subsample = 0.452086, colsample_bylevel = 0.571139, colsample_bytree = 0.720022, Score = 0.51907\n",
      "38 n_estimators = 565, max_depth = 10, learning_rate = 0.0267918529, subsample = 0.510642, colsample_bylevel = 0.511964, colsample_bytree = 0.940773, Score = 0.32947\n",
      "39 n_estimators = 378, max_depth =  3, learning_rate = 0.1240507309, subsample = 0.219938, colsample_bylevel = 0.678010, colsample_bytree = 0.999816, Score = 0.33688\n",
      "40 n_estimators = 839, max_depth = 12, learning_rate = 0.0385669369, subsample = 0.266342, colsample_bylevel = 0.801249, colsample_bytree = 0.961586, Score = 0.33678\n",
      "41 n_estimators = 437, max_depth = 11, learning_rate = 0.0233644625, subsample = 0.321998, colsample_bylevel = 0.561755, colsample_bytree = 0.960961, Score = 0.32762\n",
      "42 n_estimators = 923, max_depth = 28, learning_rate = 0.0725801194, subsample = 0.334403, colsample_bylevel = 0.619528, colsample_bytree = 0.731837, Score = 0.35865\n",
      "43 n_estimators = 789, max_depth = 49, learning_rate = 0.9031698804, subsample = 0.335432, colsample_bylevel = 0.493866, colsample_bytree = 0.828910, Score = 1.23522\n",
      "44 n_estimators = 475, max_depth = 13, learning_rate = 0.3570459630, subsample = 0.580113, colsample_bylevel = 0.769912, colsample_bytree = 0.752050, Score = 0.42210\n",
      "45 n_estimators = 469, max_depth = 21, learning_rate = 0.0102538360, subsample = 0.316122, colsample_bylevel = 0.555959, colsample_bytree = 0.741816, Score = 0.35384\n",
      "46 n_estimators = 502, max_depth = 38, learning_rate = 0.0111374803, subsample = 0.358754, colsample_bylevel = 0.505624, colsample_bytree = 0.973068, Score = 0.32991\n",
      "47 n_estimators = 678, max_depth = 25, learning_rate = 0.0217827608, subsample = 0.523825, colsample_bylevel = 0.808594, colsample_bytree = 0.875922, Score = 0.33293\n",
      "48 n_estimators = 926, max_depth = 39, learning_rate = 0.0417623298, subsample = 0.538666, colsample_bylevel = 0.614637, colsample_bytree = 0.715228, Score = 0.34953\n",
      "49 n_estimators = 848, max_depth = 35, learning_rate = 0.0199923328, subsample = 0.649909, colsample_bylevel = 0.618475, colsample_bytree = 0.963445, Score = 0.33089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 n_estimators = 660, max_depth = 53, learning_rate = 0.1744916417, subsample = 0.429841, colsample_bylevel = 0.894033, colsample_bytree = 0.908646, Score = 0.36765\n",
      "51 n_estimators = 323, max_depth = 53, learning_rate = 0.2742895948, subsample = 0.317018, colsample_bylevel = 0.618159, colsample_bytree = 0.861279, Score = 0.42262\n",
      "52 n_estimators = 575, max_depth =  3, learning_rate = 0.0504267387, subsample = 0.655626, colsample_bylevel = 0.572267, colsample_bytree = 0.911976, Score = 0.33836\n",
      "53 n_estimators = 646, max_depth =  6, learning_rate = 0.4478130499, subsample = 0.677061, colsample_bylevel = 0.863480, colsample_bytree = 0.964276, Score = 0.40714\n",
      "54 n_estimators = 805, max_depth =  2, learning_rate = 0.1437046472, subsample = 0.641922, colsample_bylevel = 0.468121, colsample_bytree = 0.877805, Score = 0.38581\n",
      "55 n_estimators = 864, max_depth = 38, learning_rate = 0.0257201746, subsample = 0.582692, colsample_bylevel = 0.791519, colsample_bytree = 0.905579, Score = 0.33344\n",
      "56 n_estimators = 497, max_depth =  3, learning_rate = 0.0308144687, subsample = 0.286670, colsample_bylevel = 0.416176, colsample_bytree = 0.941399, Score = 0.42473\n",
      "57 n_estimators = 359, max_depth = 38, learning_rate = 0.0148173959, subsample = 0.656984, colsample_bylevel = 0.856412, colsample_bytree = 0.706486, Score = 0.36368\n",
      "58 n_estimators = 653, max_depth = 23, learning_rate = 0.0689673741, subsample = 0.578014, colsample_bylevel = 0.440388, colsample_bytree = 0.709141, Score = 0.36057\n",
      "59 n_estimators = 936, max_depth = 28, learning_rate = 0.0139820517, subsample = 0.638780, colsample_bylevel = 0.727529, colsample_bytree = 0.852536, Score = 0.33051\n",
      "60 n_estimators = 961, max_depth = 42, learning_rate = 0.0494575173, subsample = 0.443794, colsample_bylevel = 0.435667, colsample_bytree = 0.842521, Score = 0.34863\n",
      "61 n_estimators = 941, max_depth = 37, learning_rate = 0.0795945844, subsample = 0.639516, colsample_bylevel = 0.814327, colsample_bytree = 0.714473, Score = 0.37009\n",
      "62 n_estimators = 316, max_depth = 25, learning_rate = 0.8171440833, subsample = 0.503361, colsample_bylevel = 0.870639, colsample_bytree = 0.811066, Score = 0.68252\n",
      "63 n_estimators = 608, max_depth = 20, learning_rate = 0.0122782175, subsample = 0.615252, colsample_bylevel = 0.513641, colsample_bytree = 0.788186, Score = 0.33737\n",
      "64 n_estimators = 326, max_depth =  8, learning_rate = 0.0466216280, subsample = 0.684720, colsample_bylevel = 0.679378, colsample_bytree = 0.901412, Score = 0.32392\n",
      "65 n_estimators = 732, max_depth = 34, learning_rate = 0.1545358630, subsample = 0.466161, colsample_bylevel = 0.600334, colsample_bytree = 0.730037, Score = 0.36813\n",
      "66 n_estimators = 484, max_depth =  2, learning_rate = 0.5748964666, subsample = 0.622408, colsample_bylevel = 0.518681, colsample_bytree = 0.773039, Score = 0.35794\n",
      "67 n_estimators = 612, max_depth = 32, learning_rate = 0.1991645342, subsample = 0.272800, colsample_bylevel = 0.874424, colsample_bytree = 0.922642, Score = 0.38911\n",
      "68 n_estimators = 373, max_depth = 26, learning_rate = 0.8023382663, subsample = 0.627920, colsample_bylevel = 0.430611, colsample_bytree = 0.800935, Score = 0.53953\n",
      "69 n_estimators = 338, max_depth = 45, learning_rate = 0.1151012629, subsample = 0.572554, colsample_bylevel = 0.749211, colsample_bytree = 0.804116, Score = 0.35725\n",
      "70 n_estimators = 375, max_depth = 10, learning_rate = 0.0912986042, subsample = 0.632452, colsample_bylevel = 0.457623, colsample_bytree = 0.995132, Score = 0.34907\n",
      "71 n_estimators = 654, max_depth = 17, learning_rate = 0.3985748388, subsample = 0.610973, colsample_bylevel = 0.404381, colsample_bytree = 0.996095, Score = 0.42605\n",
      "72 n_estimators = 876, max_depth = 52, learning_rate = 0.3130205122, subsample = 0.352322, colsample_bylevel = 0.614967, colsample_bytree = 0.947566, Score = 0.42314\n",
      "73 n_estimators = 840, max_depth = 11, learning_rate = 0.0827328462, subsample = 0.549684, colsample_bylevel = 0.652479, colsample_bytree = 0.931786, Score = 0.34405\n",
      "74 n_estimators = 863, max_depth = 11, learning_rate = 0.2574509349, subsample = 0.306930, colsample_bylevel = 0.620301, colsample_bytree = 0.804716, Score = 0.42218\n",
      "75 n_estimators = 876, max_depth = 44, learning_rate = 0.2955270232, subsample = 0.686000, colsample_bylevel = 0.527141, colsample_bytree = 0.725916, Score = 0.41859\n",
      "76 n_estimators = 494, max_depth = 48, learning_rate = 0.0183072819, subsample = 0.662914, colsample_bylevel = 0.446013, colsample_bytree = 0.717283, Score = 0.35233\n",
      "77 n_estimators = 985, max_depth =  3, learning_rate = 0.0718847944, subsample = 0.285630, colsample_bylevel = 0.790567, colsample_bytree = 0.925351, Score = 0.32769\n",
      "78 n_estimators = 618, max_depth = 21, learning_rate = 0.0132333903, subsample = 0.260413, colsample_bylevel = 0.831675, colsample_bytree = 0.813305, Score = 0.33106\n",
      "79 n_estimators = 578, max_depth =  2, learning_rate = 0.5614038526, subsample = 0.470569, colsample_bylevel = 0.406692, colsample_bytree = 0.960557, Score = 0.36013\n",
      "80 n_estimators = 453, max_depth = 39, learning_rate = 0.2792367957, subsample = 0.428404, colsample_bylevel = 0.791115, colsample_bytree = 0.846710, Score = 0.39964\n",
      "81 n_estimators = 341, max_depth =  5, learning_rate = 0.0286532695, subsample = 0.575951, colsample_bylevel = 0.524058, colsample_bytree = 0.748711, Score = 0.33779\n",
      "82 n_estimators = 704, max_depth = 38, learning_rate = 0.0212829787, subsample = 0.262961, colsample_bylevel = 0.856746, colsample_bytree = 0.988876, Score = 0.33093\n",
      "83 n_estimators = 457, max_depth = 38, learning_rate = 0.0356493505, subsample = 0.406002, colsample_bylevel = 0.482871, colsample_bytree = 0.903674, Score = 0.33723\n",
      "84 n_estimators = 525, max_depth = 33, learning_rate = 0.2113503066, subsample = 0.651353, colsample_bylevel = 0.720116, colsample_bytree = 0.765657, Score = 0.37206\n",
      "85 n_estimators = 884, max_depth = 38, learning_rate = 0.0806559339, subsample = 0.578291, colsample_bylevel = 0.514824, colsample_bytree = 0.898813, Score = 0.34954\n",
      "86 n_estimators = 542, max_depth = 48, learning_rate = 0.0897100196, subsample = 0.672743, colsample_bylevel = 0.768915, colsample_bytree = 0.761242, Score = 0.35483\n",
      "87 n_estimators = 657, max_depth = 44, learning_rate = 0.0216305583, subsample = 0.600683, colsample_bylevel = 0.493772, colsample_bytree = 0.998632, Score = 0.33300\n",
      "88 n_estimators = 550, max_depth = 11, learning_rate = 0.0118268256, subsample = 0.414797, colsample_bylevel = 0.468924, colsample_bytree = 0.920813, Score = 0.32904\n",
      "89 n_estimators = 860, max_depth = 55, learning_rate = 0.0146506694, subsample = 0.533267, colsample_bylevel = 0.703759, colsample_bytree = 0.924415, Score = 0.32928\n",
      "90 n_estimators = 876, max_depth = 30, learning_rate = 0.1434107525, subsample = 0.518028, colsample_bylevel = 0.426024, colsample_bytree = 0.959858, Score = 0.35903\n",
      "91 n_estimators = 403, max_depth = 51, learning_rate = 0.2341096236, subsample = 0.348113, colsample_bylevel = 0.444010, colsample_bytree = 0.947844, Score = 0.39720\n",
      "92 n_estimators = 859, max_depth = 10, learning_rate = 0.0111051573, subsample = 0.362207, colsample_bylevel = 0.870122, colsample_bytree = 0.980396, Score = 0.32133\n",
      "93 n_estimators = 815, max_depth = 51, learning_rate = 0.0161951484, subsample = 0.214376, colsample_bylevel = 0.407823, colsample_bytree = 0.740139, Score = 0.33541\n",
      "94 n_estimators = 560, max_depth = 41, learning_rate = 0.1017657375, subsample = 0.579904, colsample_bylevel = 0.827769, colsample_bytree = 0.747174, Score = 0.35405\n",
      "95 n_estimators = 597, max_depth = 14, learning_rate = 0.0184553028, subsample = 0.597537, colsample_bylevel = 0.747206, colsample_bytree = 0.975083, Score = 0.32881\n",
      "96 n_estimators = 349, max_depth = 28, learning_rate = 0.0265883410, subsample = 0.392322, colsample_bylevel = 0.667323, colsample_bytree = 0.948886, Score = 0.32822\n",
      "97 n_estimators = 909, max_depth = 47, learning_rate = 0.0362947251, subsample = 0.337482, colsample_bylevel = 0.654701, colsample_bytree = 0.864936, Score = 0.33585\n",
      "98 n_estimators = 378, max_depth =  2, learning_rate = 0.0272153858, subsample = 0.257277, colsample_bylevel = 0.802665, colsample_bytree = 0.703375, Score = 0.54857\n",
      "99 n_estimators = 369, max_depth = 22, learning_rate = 0.0345952079, subsample = 0.478914, colsample_bylevel = 0.768151, colsample_bytree = 0.711363, Score = 0.34617\n",
      "(100, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>colsample_bylevel</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>epoch</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>score</th>\n",
       "      <th>subsample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.870122</td>\n",
       "      <td>0.980396</td>\n",
       "      <td>92</td>\n",
       "      <td>0.011105</td>\n",
       "      <td>10</td>\n",
       "      <td>859</td>\n",
       "      <td>0.321325</td>\n",
       "      <td>0.362207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.679378</td>\n",
       "      <td>0.901412</td>\n",
       "      <td>64</td>\n",
       "      <td>0.046622</td>\n",
       "      <td>8</td>\n",
       "      <td>326</td>\n",
       "      <td>0.323925</td>\n",
       "      <td>0.684720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.547196</td>\n",
       "      <td>0.854189</td>\n",
       "      <td>35</td>\n",
       "      <td>0.028520</td>\n",
       "      <td>8</td>\n",
       "      <td>855</td>\n",
       "      <td>0.324446</td>\n",
       "      <td>0.402216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.684372</td>\n",
       "      <td>0.971310</td>\n",
       "      <td>32</td>\n",
       "      <td>0.016790</td>\n",
       "      <td>34</td>\n",
       "      <td>554</td>\n",
       "      <td>0.326180</td>\n",
       "      <td>0.425722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.831264</td>\n",
       "      <td>0.867055</td>\n",
       "      <td>27</td>\n",
       "      <td>0.025435</td>\n",
       "      <td>54</td>\n",
       "      <td>385</td>\n",
       "      <td>0.326442</td>\n",
       "      <td>0.266720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.845876</td>\n",
       "      <td>0.966426</td>\n",
       "      <td>6</td>\n",
       "      <td>0.019466</td>\n",
       "      <td>57</td>\n",
       "      <td>315</td>\n",
       "      <td>0.327296</td>\n",
       "      <td>0.313923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.435530</td>\n",
       "      <td>0.907742</td>\n",
       "      <td>5</td>\n",
       "      <td>0.021748</td>\n",
       "      <td>8</td>\n",
       "      <td>462</td>\n",
       "      <td>0.327548</td>\n",
       "      <td>0.601801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.561755</td>\n",
       "      <td>0.960961</td>\n",
       "      <td>41</td>\n",
       "      <td>0.023364</td>\n",
       "      <td>11</td>\n",
       "      <td>437</td>\n",
       "      <td>0.327625</td>\n",
       "      <td>0.321998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.790567</td>\n",
       "      <td>0.925351</td>\n",
       "      <td>77</td>\n",
       "      <td>0.071885</td>\n",
       "      <td>3</td>\n",
       "      <td>985</td>\n",
       "      <td>0.327687</td>\n",
       "      <td>0.285630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.667323</td>\n",
       "      <td>0.948886</td>\n",
       "      <td>96</td>\n",
       "      <td>0.026588</td>\n",
       "      <td>28</td>\n",
       "      <td>349</td>\n",
       "      <td>0.328221</td>\n",
       "      <td>0.392322</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    colsample_bylevel  colsample_bytree  epoch  learning_rate  max_depth  \\\n",
       "92           0.870122          0.980396     92       0.011105         10   \n",
       "64           0.679378          0.901412     64       0.046622          8   \n",
       "35           0.547196          0.854189     35       0.028520          8   \n",
       "32           0.684372          0.971310     32       0.016790         34   \n",
       "27           0.831264          0.867055     27       0.025435         54   \n",
       "6            0.845876          0.966426      6       0.019466         57   \n",
       "5            0.435530          0.907742      5       0.021748          8   \n",
       "41           0.561755          0.960961     41       0.023364         11   \n",
       "77           0.790567          0.925351     77       0.071885          3   \n",
       "96           0.667323          0.948886     96       0.026588         28   \n",
       "\n",
       "    n_estimators     score  subsample  \n",
       "92           859  0.321325   0.362207  \n",
       "64           326  0.323925   0.684720  \n",
       "35           855  0.324446   0.402216  \n",
       "32           554  0.326180   0.425722  \n",
       "27           385  0.326442   0.266720  \n",
       "6            315  0.327296   0.313923  \n",
       "5            462  0.327548   0.601801  \n",
       "41           437  0.327625   0.321998  \n",
       "77           985  0.327687   0.285630  \n",
       "96           349  0.328221   0.392322  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradient Boosting Machine 패키지인 XGBoost를 가져옵니다.\n",
    "# 이를 xgb라는 축약어로 사용합니다.\n",
    "import xgboost as xgb\n",
    "\n",
    "# scikit-learn 패키지의 model_selection 모듈에 있는 cross_val_score 함수를 가지고 옵니다.\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# 랜덤 서치를 반복할 횟수입니다.\n",
    "# 보통 100번을 반복합니다.\n",
    "num_epoch = 100\n",
    "\n",
    "# hyperparameter 탐색 결과를 리스트로 저장합니다.\n",
    "finer_hyperparameters_list = []\n",
    "\n",
    "# num_epoch 횟수만큼 랜덤 서치를 반복합니다.\n",
    "for epoch in range(num_epoch):\n",
    "    # 300에서 1000 사이의 정수형(int) 값을 랜덤하게 생성하여 n_estimators 변수에 할당합니다.\n",
    "    n_estimators = np.random.randint(low=300, high=1000)\n",
    "\n",
    "    # 2에서 60 사이의 정수형(int) 값을 랜덤하게 생성하여 max_depth 변수에 할당합니다.\n",
    "    max_depth = np.random.randint(low=2, high=60)\n",
    "\n",
    "    # 1.0에서 1-e2(10의 -2승)사이의 실수형(float) 값을 랜덤하게 생성하여  learning_rate 변수에 할당합니다.\n",
    "    learning_rate = 10 ** -np.random.uniform(low=0, high=2)\n",
    "\n",
    "    # 0.2에서 0.7사이의 실수형(float) 값을 랜덤하게 생성하여 subsample 변수에 할당합니다.\n",
    "    subsample = np.random.uniform(low=0.2, high=0.7)\n",
    "\n",
    "    # 0.7에서 1.0사이의 실수형(float) 값을 랜덤하게 생성하여 colsample_bytree 변수에 할당합니다.\n",
    "    colsample_bytree = np.random.uniform(low=0.7, high=1.0)\n",
    "\n",
    "    # 0.4에서 1.0사이의 실수형(float) 값을 랜덤하게 생성하여 colsample_bylevel 변수에 할당합니다.\n",
    "    colsample_bylevel = np.random.uniform(low=0.4, high=1.0)\n",
    "\n",
    "    # XGBRegressor를 생성합니다. 실행할때는 다음의 옵션이 들어갑니다.\n",
    "    # 1) n_estimators. 트리의 갯수입니다. 지정한 갯수만큼 트리를 생성합니다.\n",
    "    # 2) max_depth. 트리의 깊이입니다. 지정한 숫자만큼 트리가 깊게 가지를 뻗습니다.\n",
    "    # 3) learning_rate. 각 트리마다의 비중을 나타냅니다. 너무 작으면 과적합(overfitting)될 가능성이 있고, 너무 높으면 부적합(underfitting)될 가능성이 있습니다.\n",
    "    # 4) subsample. 하나의 트리를 만들 때 사용할 데이터의 비율을 나타냅니다. 0.0 ~ 1.0 사이의 값을 넣으면 지정한 비율만큼만 랜덤하게 데이터를 사용합니다.\n",
    "    # 5) colsample_bytree. 하나의 트리를 만들 때 사용할 feature의 비율을 나타냅니다. 0.0 ~ 1.0 사이의 값을 넣으면 트리를 만들 때 지정한 비율만큼만 랜덤하게 feature를 사용합니다.\n",
    "    # 6) colsample_bylevel. 트리가 한 번 가지를 칠 때 사용할 feature의 비율을 나타냅니다. 0.0 ~ 1.0 사이의 값을 넣으면 트리가 가지를 칠 때 지정한 비율만큼만 랜덤하게 feature를 사용합니다.\n",
    "    # 생성한 XGBRegressor를 model이라는 이름의 변수에 대입합니다.\n",
    "    model = xgb.XGBRegressor(n_estimators=n_estimators,\n",
    "                             max_depth=max_depth,\n",
    "                             learning_rate=learning_rate,\n",
    "                             subsample=subsample,\n",
    "                             colsample_bylevel=colsample_bylevel,\n",
    "                             colsample_bytree=colsample_bytree,\n",
    "                             seed=37)\n",
    "\n",
    "    # cross_val_score를 실행합니다. 실행할 때는 다음의 옵션이 들어갑니다.\n",
    "    # 1) model. 점수를 측정할 머신러닝 모델이 들어갑니다.\n",
    "    # 2) X_train. train 데이터의 feature 입니다.\n",
    "    # 3) y_train. train 데이터의 label 입니다.\n",
    "    # 4) cv. Cross Validation에서 데이터를 조각낼(split) 갯수입니다. 총 20조각을 내야하기 때문에 20을 대입합니다.\n",
    "    # 5) scoring. 점수를 측정할 공식입니다. 앞서 구현한 RMSE를 적용합니다.\n",
    "    # 마지막으로, 이 함수의 실행 결과의 평균(mean)을 구한 뒤 score라는 이름의 새로운 변수에 할당합니다.\n",
    "    score = cross_val_score(model, X_train, y_train, cv=20, scoring=rmse_score).mean()\n",
    "\n",
    "    # hyperparameter 탐색 결과를 딕셔너리화 합니다.\n",
    "    hyperparameters = {\n",
    "        'epoch': epoch,\n",
    "        'score': score,\n",
    "        'n_estimators': n_estimators,\n",
    "        'max_depth': max_depth,\n",
    "        'learning_rate': learning_rate,\n",
    "        'subsample': subsample,\n",
    "        'colsample_bylevel': colsample_bylevel,\n",
    "        'colsample_bytree': colsample_bytree,\n",
    "    }\n",
    "\n",
    "    # hyperparameter 탐색 결과를 리스트에 저장합니다.\n",
    "    finer_hyperparameters_list.append(hyperparameters)\n",
    "\n",
    "    # hyperparameter 탐색 결과를 출력합니다.\n",
    "    print(f\"{epoch:2} n_estimators = {n_estimators}, max_depth = {max_depth:2}, learning_rate = {learning_rate:.10f}, subsample = {subsample:.6f}, colsample_bylevel = {colsample_bylevel:.6f}, colsample_bytree = {colsample_bytree:.6f}, Score = {score:.5f}\")\n",
    "\n",
    "# finer_hyperparameters_list를 Pandas의 DataFrame으로 변환합니다.\n",
    "finer_hyperparameters_list = pd.DataFrame.from_dict(finer_hyperparameters_list)\n",
    "\n",
    "# 변환한 finer_hyperparameters_list를 score가 낮은 순으로 정렬합니다.\n",
    "# (RMSE는 score가 낮을 수록 더 정확도가 높다고 가정합니다)\n",
    "finer_hyperparameters_list = finer_hyperparameters_list.sort_values(by=\"score\")\n",
    "\n",
    "# finer_hyperparameters_list 변수에 할당된 데이터의 행렬 사이즈를 출력합니다.\n",
    "# 출력은 (row, column) 으로 표시됩니다.\n",
    "print(finer_hyperparameters_list.shape)\n",
    "\n",
    "# finer_hyperparameters_list의 상위 10개를 출력합니다.\n",
    "finer_hyperparameters_list.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "탐색 결과 다음의 하이퍼패러미터가 가장 좋은 하이퍼패러미터라는 사실을 발견할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators(best) = 859, max_depth(best) = 10, learning_rate(best) = 0.011105, subsample(best) = 0.362207, colsample_bytree(best) = 0.980396, colsample_bylevel(best) = 0.870122\n"
     ]
    }
   ],
   "source": [
    "# 가장 score가 낮게 나온(=좋은 정확도가 나온) 하이퍼패러미터를 가져옵니다.\n",
    "# 이를 best_hyperparameters라는 이름의 변수에 저장합니다.\n",
    "best_hyperparameters = finer_hyperparameters_list.iloc[0]\n",
    "\n",
    "# best_hyperparameters에서 n_estimators 하이퍼패러미터만 가져옵니다.\n",
    "# 이를 best_n_estimators라는 이름의 변수에 저장합니다.\n",
    "# 주의: n_estimators는 무조건 정수형 값(int)이어야 하기 때문에, 정수형으로 타입 변환을 해줍니다.\n",
    "best_n_estimators = int(best_hyperparameters[\"n_estimators\"])\n",
    "\n",
    "# best_hyperparameters에서 max_depth 하이퍼패러미터만 가져옵니다.\n",
    "# 이를 best_max_depth라는 이름의 변수에 저장합니다.\n",
    "# 주의: max_depth는 무조건 정수형 값(int)이어야 하기 때문에, 정수형으로 타입 변환을 해줍니다.\n",
    "best_max_depth = int(best_hyperparameters[\"max_depth\"])\n",
    "\n",
    "# best_hyperparameters에서 learning_rate 하이퍼패러미터만 가져옵니다.\n",
    "# 이를 best_learning_rate라는 이름의 변수에 저장합니다.\n",
    "best_learning_rate = best_hyperparameters[\"learning_rate\"]\n",
    "\n",
    "# best_hyperparameters에서 subsample 하이퍼패러미터만 가져옵니다.\n",
    "# 이를 best_subsample라는 이름의 변수에 저장합니다.\n",
    "best_subsample = best_hyperparameters[\"subsample\"]\n",
    "\n",
    "# best_hyperparameters에서 colsample_bytree 하이퍼패러미터만 가져옵니다.\n",
    "# 이를 best_colsample_bytree라는 이름의 변수에 저장합니다.\n",
    "best_colsample_bytree = best_hyperparameters[\"colsample_bytree\"]\n",
    "\n",
    "# best_hyperparameters에서 colsample_bylevel 하이퍼패러미터만 가져옵니다.\n",
    "# 이를 best_colsample_bylevel라는 이름의 변수에 저장합니다.\n",
    "best_colsample_bylevel = best_hyperparameters[\"colsample_bylevel\"]\n",
    "\n",
    "# best_hyperparameters들을 출력합니다.\n",
    "print(f\"n_estimators(best) = {best_n_estimators}, max_depth(best) = {best_max_depth}, learning_rate(best) = {best_learning_rate:.6f}, subsample(best) = {best_subsample:.6f}, colsample_bytree(best) = {best_colsample_bytree:.6f}, colsample_bylevel(best) = {best_colsample_bylevel:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Gradient Boosting Machine\n",
    "\n",
    "Hyperparameter Tuning으로 만족스러운 하이퍼패러미터를 찾았다면, 이제 이 하이퍼패러미터를 활용하여 머신러닝 모델을 학습할 시간입니다.\n",
    "\n",
    "이번에 사용할 알고리즘은 그래디언트 부스팅 머신(Gradient Boosting Machine)입니다. 그래디언트 부스팅 머신은 의사결정나무(Decision Tree)에 그래디언트 부스팅(Boosting Machine)이라는 알고리즘을 적용한 모델인데, 구조화된 데이터(Structured Data)에 한해서는 가장 강력한 머신러닝 알고리즘이라고 불리우고 있습니다. 알고리즘의 동작 원리는 다음과 같습니다.\n",
    "\n",
    "  1. 의사결정나무(Decision Tree)를 하나 학습합니다.\n",
    "  1. 1번에서 학습한 의사결정나무를 통해, 학습(train)데이터를 예측합니다. 그리고 예측값과 정답의 차이(residual)를 계산합니다.\n",
    "  1. 위 차이(residual)를 보정하는 또 하나의 의사결정나무(Decision Tree)를 학습합니다. 두 번째 의사결정나무에서는 차이를 입력값으로 받고, 차이를 보정하기 위해서는 얼만큼의 보정값이 필요한지를 예측합니다.\n",
    "  1. 위 방식을 끊임없이 반복합니다.\n",
    "  \n",
    "이러한 방식을 거치면 의사결정나무(Decision Tree)보다 더 강력한 알고리즘을 구현할 수 있습니다. 자세한 설명은 다음의 링크들을 참고해주세요.\n",
    "\n",
    "  * [A Kaggle Master Explains Gradient Boosting](http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/)\n",
    "  * [Gradient Boosting from scratch](https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d)\n",
    "  * [A Gentle Introduction to the Gradient Boosting Algorithm for Machine Learning](https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/)\n",
    "\n",
    "이번에는 가장 강력한 그래디언트 부스팅 머신(Gradient Boosting Machine) 구현체중 하나인 [XGBoost](https://github.com/dmlc/xgboost)를 사용하겠습니다. XGBoost의 회귀(Regression)용 머신러닝 모델인 ```XGBRegressor```를 가져올텐데, 이 ```XGBRegressor```에는 크게 두 가지 기능이 있습니다.\n",
    "\n",
    "  * **fit**: 머신러닝 알고리즘을 학습시킵니다. 전문용어로 fitting한다고 하기 때문에 fit이라는 표현을 사용합니다. fit을 하기 위해서는 train 데이터가 필요하며, 정확히는 train 데이터의 feature(X_train)와 label(y_train)이 필요합니다.\n",
    "  * **predict**: **fit**이 끝나면, 이후에 **predict**를 통해 예측을 할 수 있습니다. predict를 하기 위해서는 test 데이터가 필요하며, 정확히는 test 데이터의 feature(X_test)가 필요합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, colsample_bylevel=0.87012248132277636,\n",
       "       colsample_bytree=0.98039571556042127, gamma=0,\n",
       "       learning_rate=0.011105157251072018, max_delta_step=0, max_depth=10,\n",
       "       min_child_weight=1, missing=None, n_estimators=859, nthread=-1,\n",
       "       objective='reg:linear', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=37, silent=True,\n",
       "       subsample=0.36220688378150445)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradient Boosting Machine 패키지인 XGBoost를 가져옵니다.\n",
    "# 이를 xgb라는 축약어로 사용합니다.\n",
    "import xgboost as xgb\n",
    "\n",
    "# 주의: 혹시 하이퍼패러미터 튜닝을 하는데 시간이 너무 오래 걸린다면,\n",
    "# 이를 대신해서 다음의 하이퍼패러미터를 사용해주세요. (아래 줄의 주석을 풀면 됩니다)\n",
    "# best_n_estimators = 859\n",
    "# best_max_depth = 10\n",
    "# best_learning_rate = 0.011105\n",
    "# best_subsample = 0.362207\n",
    "# best_colsample_bytree = 0.980396\n",
    "# best_colsample_bylevel = 0.870122\n",
    "\n",
    "# XGBRegressor를 생성합니다. 실행할때는 다음의 옵션이 들어갑니다.\n",
    "# 1) n_estimators. 트리의 갯수입니다. 지정한 갯수만큼 트리를 생성합니다.\n",
    "# 2) max_depth. 트리의 깊이입니다. 지정한 숫자만큼 트리가 깊게 가지를 뻗습니다.\n",
    "# 3) learning_rate. 각 트리마다의 비중을 나타냅니다. 너무 작으면 과적합(overfitting)될 가능성이 있고, 너무 높으면 부적합(underfitting)될 가능성이 있습니다.\n",
    "# 4) subsample. 하나의 트리를 만들 때 사용할 데이터의 비율을 나타냅니다. 0.0 ~ 1.0 사이의 값을 넣으면 지정한 비율만큼만 랜덤하게 데이터를 사용합니다.\n",
    "# 5) colsample_bytree. 하나의 트리를 만들 때 사용할 feature의 비율을 나타냅니다. 0.0 ~ 1.0 사이의 값을 넣으면 트리를 만들 때 지정한 비율만큼만 랜덤하게 feature를 사용합니다.\n",
    "# 6) colsample_bylevel. 트리가 한 번 가지를 칠 때 사용할 feature의 비율을 나타냅니다. 0.0 ~ 1.0 사이의 값을 넣으면 트리가 가지를 칠 때 지정한 비율만큼만 랜덤하게 feature를 사용합니다.\n",
    "# 생성한 XGBRegressor를 model이라는 이름의 변수에 대입합니다.\n",
    "model = xgb.XGBRegressor(n_estimators=best_n_estimators,\n",
    "                         max_depth=best_max_depth,\n",
    "                         learning_rate=best_learning_rate,\n",
    "                         subsample=best_subsample,\n",
    "                         colsample_bytree=best_colsample_bytree,\n",
    "                         colsample_bylevel=best_colsample_bylevel,\n",
    "                         seed=37)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit\n",
    "\n",
    "이제 앞서 설명한 머신러닝 모델을 학습시켜보겠습니다. 머신러닝 모델을 학습시킬때는 ```fit``` 함수를 사용합니다. 학습을 할 때는 1) train 데이터의 feature인 ```X_train```, 그리고 2) train 데이터의 label인 ```y_train```이 필요합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, colsample_bylevel=0.87012248132277636,\n",
       "       colsample_bytree=0.98039571556042127, gamma=0,\n",
       "       learning_rate=0.011105157251072018, max_delta_step=0, max_depth=10,\n",
       "       min_child_weight=1, missing=None, n_estimators=859, nthread=-1,\n",
       "       objective='reg:linear', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=37, silent=True,\n",
       "       subsample=0.36220688378150445)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# XGBRegressor를 학습(fitting)합니다.\n",
    "# 학습에는 fit 이라는 기능을 사용하며, train 데이터의 feature(X_train)와 label(y_train)을 집어넣습니다.\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict\n",
    "\n",
    "머신러닝 모델이 성공적으로 학습이 되었다면, 남은 것은 이 모델를 활용해 test 데이터에 있는 자전거 대여량을  예측하는 것입니다. 예측은 ```model.predict```로 할 수 있으며, 이 때 test 데이터의 feature인 ```X_test```가 필요합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6493,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 2.71734762,  1.71947825,  1.47986841, ...,  4.71064758,\n",
       "        4.44729996,  3.79223394], dtype=float32)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit이 끝났으면, predict라는 기능을 사용하여 log transformation한 자전거 대여량(log_count)을 예측합니다.\n",
    "# log_predictions의 실행이 끝나면 test 데이터의 log transformation한 자전거 대여량(log_count)을 반환하며,\n",
    "# 이를 predictions라는 이름의 변수에 할당합니다.\n",
    "log_predictions = model.predict(X_test)\n",
    "\n",
    "# log_predictions 변수에 할당된 데이터의 사이즈를 출력합니다.\n",
    "# 출력은 (row, column) 으로 표시되나, column이 없기 때문에 (row,) 형태로 표시될 것입니다.\n",
    "print(log_predictions.shape)\n",
    "\n",
    "# log_predictions 변수를 출력합니다.\n",
    "log_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 데이터를 분석할 때 설명한대로, **머신러닝 모델에서 예측한 것은 자전거 대여량(count)이 아닌 log transformation한 자전거 대여량(log_count)입니다.** 이를 다시 자전거 대여량(count)으로 원상복구 하기 위해 [exp](https://en.wikipedia.org/wiki/Exponential_function)를 사용하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6493,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([  14.14011192,    4.58161545,    3.39236784, ...,  110.12409973,\n",
       "         84.39605713,   43.3553772 ], dtype=float32)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# log transformation한 자전거 대여량(log_count)을 다시 exp로 원상복귀 합니다.\n",
    "# (=자연로그는 exp로 없애버릴 수 있습니다)\n",
    "# 이를 predictions라는 새로운 변수에 할당합니다.\n",
    "predictions = np.exp(log_predictions) - 1\n",
    "\n",
    "# predictions 변수에 할당된 데이터의 사이즈를 출력합니다.\n",
    "# 출력은 (row, column) 으로 표시되나, column이 없기 때문에 (row,) 형태로 표시될 것입니다.\n",
    "print(predictions.shape)\n",
    "\n",
    "# predictions 변수를 출력합니다.\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit\n",
    "\n",
    "머신러닝 모델의 fit과 predict 를 통해 우리는 test 데이터에 있는 자전거 대여량(count)을 예측하였습니다. 이제 우리에게 남은 건 이를 캐글([kaggle](http://kaggle.com/))이 권장하는 제출(submission) 포멧에 맞게 정리한 뒤 파일로 저장하는 것입니다.\n",
    "\n",
    "캐글의 [Bike Sharing Demand](https://www.kaggle.com/c/bike-sharing-demand) 경진대회에서는 **sampleSubmission.csv**라는 제출 포멧을 제공합니다. ([다운로드 링크](https://www.kaggle.com/c/bike-sharing-demand/data)) 우리는 우리가 예측한 값을 이 제출 포멧에 맞게 집어넣고 저장할 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6493, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-01-20 00:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-01-20 01:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-01-20 02:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-01-20 03:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-01-20 04:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              datetime  count\n",
       "0  2011-01-20 00:00:00      0\n",
       "1  2011-01-20 01:00:00      0\n",
       "2  2011-01-20 02:00:00      0\n",
       "3  2011-01-20 03:00:00      0\n",
       "4  2011-01-20 04:00:00      0"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 캐글이 제공하는 제출 포멧(sampleSubmission.csv)을 읽어옵니다.\n",
    "# 이를 submission 이라는 이름의 변수에 할당합니다.\n",
    "submission = pd.read_csv(\"data/bike/sampleSubmission.csv\")\n",
    "\n",
    "# submission 변수에 할당된 데이터의 행렬 사이즈를 출력합니다.\n",
    "# 출력은 (row, column) 으로 표시됩니다.\n",
    "print(submission.shape)\n",
    "\n",
    "# submission 데이터의 상위 5개를 띄웁니다.\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6493, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-01-20 00:00:00</td>\n",
       "      <td>14.140112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-01-20 01:00:00</td>\n",
       "      <td>4.581615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-01-20 02:00:00</td>\n",
       "      <td>3.392368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-01-20 03:00:00</td>\n",
       "      <td>2.394218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-01-20 04:00:00</td>\n",
       "      <td>1.855429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              datetime      count\n",
       "0  2011-01-20 00:00:00  14.140112\n",
       "1  2011-01-20 01:00:00   4.581615\n",
       "2  2011-01-20 02:00:00   3.392368\n",
       "3  2011-01-20 03:00:00   2.394218\n",
       "4  2011-01-20 04:00:00   1.855429"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 제출 포멧(submission)의 자전거 대여량(count) 컬럼에 우리의 예측값(predictions)를 집어넣습니다.\n",
    "# 두 데이터 모두 길이가 6493개로 동일하기 때문에, 등호(=)를 통해 쉽게 예측값을 넣을 수 있습니다.\n",
    "submission[\"count\"] = predictions\n",
    "\n",
    "# submission 변수에 할당된 데이터의 행렬 사이즈를 출력합니다.\n",
    "# 출력은 (row, column) 으로 표시됩니다.\n",
    "print(submission.shape)\n",
    "\n",
    "# submission 데이터의 상위 5개를 띄웁니다.\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 마지막으로 submission 변수에 들어간 값을 csv 형식의 데이터로 저장합니다.\n",
    "submission.to_csv(\"data/bike/xgboost_0.37486.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 캐글의 [제출 페이지(Late Submission)](https://www.kaggle.com/c/bike-sharing-demand/submit)로 이동해 **xgboost_0.37486.csv** 파일을 제출하면 점수를 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
